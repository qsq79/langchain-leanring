"""
Demo 34: å›¾åƒæ¶ˆæ¯ - å¤šæ¨¡æ€äº¤äº’

æœ¬æ¼”ç¤ºå±•ç¤ºå¦‚ä½•:
1. å¤„ç†å›¾åƒè¾“å…¥
2. è§†è§‰ç†è§£
3. å›¾æ–‡ç»“åˆå¯¹è¯
4. å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨
5. è§†è§‰-æ–‡æœ¬æ··åˆè¾“å‡º

è¿è¡Œæ–¹å¼:
    python demo_34_image_messages.py

å‰ç½®è¦æ±‚:
    - å·²é…ç½® OPENAI_API_KEYï¼ˆéœ€è¦æ”¯æŒ Vision çš„æ¨¡å‹ï¼‰
    - å·²å®‰è£… autogen-agentchat å’Œ autogen-ext
    - ç†è§£è®°å¿†ç®¡ç†å’Œäººå·¥äº¤äº’åŸºç¡€

ç›¸å…³æ–‡æ¡£:
    - https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/multimodal.html
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
# è¿™æ ·å¯ä»¥ç›´æ¥è¿è¡Œè„šæœ¬æ–‡ä»¶ï¼Œè€Œä¸éœ€è¦ä»ç‰¹å®šç›®å½•è¿è¡Œ
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent  # å‘ä¸Š 3 çº§åˆ° autogen-learning/
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))


import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from common.config import get_settings
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¿®å¤ç¼–ç é—®é¢˜
os.environ['PYTHONIOENCODING'] = 'utf-8'


# ===== æ¼”ç¤ºå‡½æ•° =====
async def demo_image_description():
    """æ¼”ç¤º 1: å›¾åƒæè¿°"""
    print("=" * 80)
    print("æ¼”ç¤º 1: å›¾åƒæè¿°å’Œåˆ†æ")

    print("=" * 80 + "\n")

    settings = get_settings()
    model_client = OpenAIChatCompletionClient(
        model=settings.openai_model,
        api_key=settings.openai_api_key
    )

    # åˆ›å»ºè§†è§‰ Agent
    vision_agent = AssistantAgent(
        name="vision_agent",
        model_client=model_client,
        description="ä½ æ˜¯ä¸€ä¸ªè§†è§‰ç†è§£åŠ©æ‰‹ï¼Œå¯ä»¥æè¿°å’Œåˆ†æå›¾åƒå†…å®¹ã€‚"
    )

    print("ğŸ’¬ å›¾åƒæè¿°æµ‹è¯•")
    print()

    # æ¨¡æ‹Ÿå›¾åƒè¾“å…¥ï¼ˆå®é™…ä¸­ä¼šä¼ é€’çœŸå®çš„å›¾åƒæ•°æ®ï¼‰
    image_scenarios = [
        {
            "description": "ä¸€å¼ é£æ™¯ç…§ç‰‡",
            "features": ["å±±è„‰", "è“å¤©", "ç»¿è‰²æ ‘æœ¨", "æ¸…æ¾ˆçš„æ¹–æ³Š"]
        },
        {
            "description": "ä¸€å¼ åŸå¸‚è¡—é“ç…§ç‰‡",
            "features": ["ç°ä»£åŒ–å»ºç­‘", "ç¹å¿™çš„äº¤é€š", "è¡Œäºº", "å•†åº—æ‹›ç‰Œ"]
        },
        {
            "description": "ä¸€å¼ äº§å“ç…§ç‰‡",
            "features": ["åŒ…è£…ç²¾ç¾çš„äº§å“", "ç™½è‰²èƒŒæ™¯", "å“ç‰Œæ ‡å¿—", "äº§å“åç§°"]
        }
    ]

    for i, scenario in enumerate(image_scenarios, 1):
        print(f"\n{'â”€' * 40}")
        print(f"åœºæ™¯ {i}: {scenario['description']}")
        print(f"{'â”€' * 40}\n")

        # æ„å»ºå›¾åƒæè¿°ä»»åŠ¡
        task = f"""æˆ‘æœ‰ä¸€å¼ å›¾ç‰‡ï¼Œå†…å®¹æ˜¯ï¼š{scenario['description']}
        å›¾ç‰‡ä¸­åŒ…å«ä»¥ä¸‹ç‰¹å¾ï¼š{', '.join(scenario['features'])}

è¯·ï¼š
1. è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡
2. è¯†åˆ«å›¾ç‰‡ä¸­çš„ä¸»è¦å…ƒç´ 
3. åˆ†æå›¾ç‰‡çš„é£æ ¼å’Œæ°›å›´
4. æä¾›ä»»ä½•æœ‰è¶£çš„è§‚å¯Ÿ"""

        print(f"ğŸ‘¤ ä»»åŠ¡:")
        print(task[:200] + "...")
        print()

        result = await vision_agent.run(task=task)

        print(f"ğŸ¤– Agent åˆ†æ:")
        for message in result.messages:
            # é™åˆ¶è¾“å‡ºé•¿åº¦
            content = message.content[:300] + "..." if len(message.content) > 300 else message.content
            print(f"{content}")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 80 + "\n")


async def demo_text_with_image():
    """æ¼”ç¤º 2: å›¾æ–‡ç»“åˆå¯¹è¯"""
    print("=" * 80)
    print("æ¼”ç¤º 2: å›¾æ–‡ç»“åˆå¯¹è¯")
    print("=" * 80 + "\n")

    settings = get_settings()
    model_client = OpenAIChatCompletionClient(
        model=settings.openai_model,
        api_key=settings.openai_api_key
    )

    # åˆ›å»ºå¤šæ¨¡æ€ Agent
    multimodal_agent = AssistantAgent(
        name="multimodal_agent",
        model_client=model_client,
        description="ä½ æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŠ©æ‰‹ï¼Œå¯ä»¥åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ã€‚"
    )

    print("ğŸ’¬ å›¾æ–‡ç»“åˆæµ‹è¯•")
    print()

    # æµ‹è¯•åœºæ™¯
    test_cases = [
        {
            "image": "ä¸€å¼ æ˜¾ç¤ºå¤æ‚æ•°æ®å›¾è¡¨çš„å›¾ç‰‡",
            "text": "è¯·åˆ†æè¿™ä¸ªå›¾è¡¨ä¸­çš„æ•°æ®è¶‹åŠ¿"
        },
        {
            "image": "ä¸€å¼ åŒ…å«å¤šä¸ªæ­¥éª¤çš„æµç¨‹å›¾",
            "text": "æ€»ç»“è¿™ä¸ªæµç¨‹å›¾çš„å…³é”®æ­¥éª¤"
        },
        {
            "image": "ä¸€å¼ ä»£ç ç¼–è¾‘å™¨ç•Œé¢æˆªå›¾",
            "text": "æŒ‡å‡ºä»£ç ä¸­çš„æ½œåœ¨é—®é¢˜å’Œæ”¹è¿›å»ºè®®"
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'â”€' * 40}")
        print(f"æµ‹è¯• {i}: {test_case['image']}")
        print(f"{'â”€' * 40}\n")

        task = f"""å›¾åƒæè¿°ï¼š{test_case['image']}
æ–‡æœ¬é—®é¢˜ï¼š{test_case['text']}

è¯·ç»“åˆå›¾åƒå’Œæ–‡æœ¬å›ç­”é—®é¢˜ã€‚"""
        
        print(f"ğŸ‘¤ ä»»åŠ¡:")
        print(task[:150] + "...")
        print()

        result = await multimodal_agent.run(task=task)

        print(f"ğŸ¤– Agent å“åº”:")
        for message in result.messages:
            content = message.content[:250] + "..." if len(message.content) > 250 else message.content
            print(f"{content}")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 80 + "\n")


async def demo_comparison_analysis():
    """æ¼”ç¤º 3: å›¾åƒæ¯”è¾ƒåˆ†æ"""
    print("=" * 80)
    print("æ¼”ç¤º 3: å›¾åƒæ¯”è¾ƒåˆ†æ")
    print("=" * 80 + "\n")

    settings = get_settings()
    model_client = OpenAIChatCompletionClient(
        model=settings.openai_model,
        api_key=settings.openai_api_key
    )

    # åˆ›å»ºåˆ†æ Agent
    analyst_agent = AssistantAgent(
        name="analyst_agent",
        model_client=model_client,
        description="ä½ æ˜¯ä¸€ä¸ªå›¾åƒåˆ†æä¸“å®¶ï¼Œæ“…é•¿æ¯”è¾ƒå’Œåˆ†æå¤šå¼ å›¾ç‰‡ã€‚"
    )

    print("ğŸ’¬ å›¾åƒæ¯”è¾ƒæµ‹è¯•")
    print()

    # æ¨¡æ‹Ÿå¤šå¼ å›¾åƒ
    image_sets = [
        {
            "set": "äº§å“ç…§ç‰‡å¯¹æ¯”",
            "images": [
                "äº§å“ A çš„ç…§ç‰‡ï¼ˆæ­£é¢è§†è§’ï¼‰",
                "äº§å“ A çš„ç…§ç‰‡ï¼ˆä¾§é¢è§†è§’ï¼‰",
                "äº§å“ B çš„ç…§ç‰‡ï¼ˆæ­£é¢è§†è§’ï¼‰"
            ]
        },
        {
            "set": "å‰åå¯¹æ¯”ç…§ç‰‡",
            "images": [
                "ä¿®å¤å‰çš„çŠ¶æ€ç…§ç‰‡",
                "ä¿®å¤åçš„çŠ¶æ€ç…§ç‰‡"
            ]
        }
    ]

    for i, image_set in enumerate(image_sets, 1):
        print(f"\n{'â”€' * 40}")
        print(f"å›¾åƒé›† {i}: {image_set['set']}")
        print(f"{'â”€' * 40}\n")

        task = f"""åˆ†æä»¥ä¸‹å¤šå¼ å›¾ç‰‡ï¼š
{chr(10).join([f"{j+1}. {img}" for j, img in enumerate(image_set['images'], 1)])}

è¯·ï¼š
1. æ¯”è¾ƒå›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚
2. è¯†åˆ«å…³é”®å˜åŒ–
3. æä¾›åˆ†æå’Œæ€»ç»“
4. ç»™å‡ºå¯èƒ½çš„ç»“è®º"""

        print(f"ğŸ‘¤ ä»»åŠ¡:")
        print(task[:150] + "...")
        print()

        result = await analyst_agent.run(task=task)

        print(f"ğŸ¤– Agent åˆ†æ:")
        for message in result.messages:
            content = message.content[:300] + "..." if len(message.content) > 300 else message.content
            print(f"{content}")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 80 + "\n")


async def demo_visual_qa():
    """æ¼”ç¤º 4: è§†è§‰é—®ç­”"""
    print("=" * 80)
    print("æ¼”ç¤º 4: è§†è§‰é—®ç­”")
    print("=" * 80 + "\n")

    settings = get_settings()
    model_client = OpenAIChatCompletionClient(
        model=settings.openai_model,
        api_key=settings.openai_api_key
    )

    # åˆ›å»ºé—®ç­” Agent
    qa_agent = AssistantAgent(
        name="visual_qa_agent",
        model_client=model_client,
        description="ä½ æ˜¯ä¸€ä¸ªè§†è§‰é—®ç­”ä¸“å®¶ï¼Œèƒ½å¤ŸåŸºäºå›¾åƒå›ç­”ç›¸å…³é—®é¢˜ã€‚"
    )

    print("ğŸ’¬ è§†è§‰é—®ç­”æµ‹è¯•")
    print()

    # é—®ç­”åœºæ™¯
    qa_scenarios = [
        {
            "image": "ä¸€å¼ åŒ…å«å¤šç§æ°´æœçš„å›¾ç‰‡",
            "questions": ["å›¾ç‰‡ä¸­æœ‰å‡ ç§æ°´æœï¼Ÿ", "å®ƒä»¬åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ", "ä¸»è¦æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"]
        },
        {
            "image": "ä¸€å¼ åŠå…¬å®¤å¸ƒå±€å›¾",
            "questions": ["æ¡Œæ¤…å¦‚ä½•æ’åˆ—ï¼Ÿ", "æœ‰å‡ ä¸ªå·¥ä½œç«™ï¼Ÿ", "æœ‰ä»€ä¹ˆåŠå…¬è®¾å¤‡ï¼Ÿ"]
        },
        {
            "image": "ä¸€å¼ äº¤é€šæ ‡å¿—å›¾",
            "questions": ["è¿™æ˜¯ä»€ä¹ˆæ ‡å¿—ï¼Ÿ", "å®ƒçš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ", "åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹ä½¿ç”¨ï¼Ÿ"]
        }
    ]

    for i, scenario in enumerate(qa_scenarios, 1):
        print(f"\n{'â”€' * 40}")
        print(f"åœºæ™¯ {i}: {scenario['image']}")
        print(f"{'â”€' * 40}\n")

        # æ„å»ºé—®ç­”ä»»åŠ¡
        questions_text = "\n".join([
            f"{j+1}. {q}" for j, q in enumerate(scenario['questions'], 1)
        ])

        task = f"""å›¾åƒæè¿°ï¼š{scenario['image']}

è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
{questions_text}"""

        print(f"ğŸ‘¤ ä»»åŠ¡:")
        print(task[:200] + "...")
        print()

        result = await qa_agent.run(task=task)

        print(f"ğŸ¤– Agent ç­”æ¡ˆ:")
        for message in result.messages:
            content = message.content[:300] + "..." if len(message.content) > 300 else message.content
            print(f"{content}")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 80 + "\n")


async def demo_document_understanding():
    """æ¼”ç¤º 5: æ–‡æ¡£ç†è§£"""
    print("=" * 80)
    print("æ¼”ç¤º 5: æ–‡æ¡£ç†è§£")
    print("=" * 80 + "\n")

    settings = get_settings()
    model_client = OpenAIChatCompletionClient(
        model=settings.openai_model,
        api_key=settings.openai_api_key
    )

    # åˆ›å»ºæ–‡æ¡£ç†è§£ Agent
    doc_agent = AssistantAgent(
        name="doc_understanding_agent",
        model_client=model_client,
        description="ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç†è§£ä¸“å®¶ï¼Œå¯ä»¥è¯»å–å’Œåˆ†ææ–‡æ¡£å›¾ç‰‡ã€‚"
    )

    print("ğŸ’¬ æ–‡æ¡£ç†è§£æµ‹è¯•")
    print()

    # æ–‡æ¡£åœºæ™¯
    document_scenarios = [
        {
            "type": "å‘ç¥¨",
            "image": "ä¸€å¼ å‘ç¥¨çš„æ‰«æå›¾ç‰‡",
            "info": "éœ€è¦æå–ï¼šå‘ç¥¨å·ç ã€æ—¥æœŸã€é‡‘é¢ã€é¡¹ç›®"
        },
        {
            "type": "è¡¨æ ¼",
            "image": "ä¸€å¼ å¤æ‚æ•°æ®è¡¨æ ¼å›¾ç‰‡",
            "info": "éœ€è¦æå–ï¼šè¡¨æ ¼ç»“æ„ã€æ•°æ®å†…å®¹ã€å…³é”®ä¿¡æ¯"
        },
        {
            "type": "å›¾è¡¨",
            "image": "ä¸€å¼ åŒ…å«å¤šä¸ªå›¾è¡¨çš„å›¾ç‰‡",
            "info": "éœ€è¦æå–ï¼šæ¯ä¸ªå›¾è¡¨çš„ç±»å‹ã€æ•°æ®è¶‹åŠ¿ã€ç»“è®º"
        }
    ]

    for i, doc_scenario in enumerate(document_scenarios, 1):
        print(f"\n{'â”€' * 40}")
        print(f"åœºæ™¯ {i}: {doc_scenario['type']}æ–‡æ¡£")
        print(f"{'â”€' * 40}\n")

        task = f"""æ–‡æ¡£ç±»å‹ï¼š{doc_scenario['type']}
æ–‡æ¡£æè¿°ï¼š{doc_scenario['image']}

éœ€è¦æå–çš„ä¿¡æ¯ï¼š
{doc_scenario['info']}

è¯·ä»”ç»†åˆ†ææ–‡æ¡£å¹¶æå–æ‰€éœ€ä¿¡æ¯ã€‚"""

        print(f"ğŸ‘¤ ä»»åŠ¡:")
        print(task[:150] + "...")
        print()

        result = await doc_agent.run(task=task)

        print(f"ğŸ¤– Agent åˆ†æ:")
        for message in result.messages:
            content = message.content[:300] + "..." if len(message.content) > 300 else message.content
            print(f"{content}")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆ")
    print("=" * 80 + "\n")


# ===== ä¸»å‡½æ•° =====
async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                â•‘
â•‘          AutoGen 0.4+ - å›¾åƒæ¶ˆæ¯æ¼”ç¤º              â•‘
â•‘           Multimodal Image Messages                    â•‘
â•‘                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    print("=" * 80 + "\n")

    try:
        # æ£€æŸ¥ API Key
        settings = get_settings()
        if not settings.openai_api_key:
            print("âŒ é”™è¯¯: æœªé…ç½® OPENAI_API_KEY")
            print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")
            return

        # æ¼”ç¤º 1: å›¾åƒæè¿°
        await demo_image_description()

        # æ¼”ç¤º 2: å›¾æ–‡ç»“åˆ
        await demo_text_with_image()

        # æ¼”ç¤º 3: å›¾åƒæ¯”è¾ƒ
        await demo_comparison_analysis()

        # æ¼”ç¤º 4: è§†è§‰é—®ç­”
        await demo_visual_qa()

        # æ¼”ç¤º 5: æ–‡æ¡£ç†è§£
        await demo_document_understanding()

        print("=" * 80)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)
        print("\nå…³é”®è¦ç‚¹:")
        print("  âœ“ å¤šæ¨¡æ€å¤„ç†ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥")
        print("  âœ“ è§†è§‰ç†è§£å¯ä»¥åˆ†æå›¾åƒå†…å®¹å’Œç‰¹å¾")
        print("  âœ“ å›¾æ–‡ç»“åˆæä¾›äº†æ›´ä¸°å¯Œçš„äº¤äº’æ–¹å¼")
        print("  âœ“ å›¾åƒæ¯”è¾ƒå¯ä»¥å‘ç°å·®å¼‚å’Œå˜åŒ–")
        print("  âœ“ è§†è§‰é—®ç­”å¯ä»¥å®ç°å›¾ç‰‡é—®ç­”åŠŸèƒ½")
        print("  âœ“ æ–‡æ¡£ç†è§£å¯ä»¥æå–ç»“æ„åŒ–ä¿¡æ¯")
        print()
        print("æ³¨æ„äº‹é¡¹:")
        print("  - éœ€è¦æ”¯æŒ Vision çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4Vï¼‰")
        print("  - å›¾åƒæ•°æ®é€šè¿‡æ¶ˆæ¯ç±»å‹ä¼ é€’")
        print("  - å¯ä»¥ä¸å…¶ä»–åŠŸèƒ½ï¼ˆè®°å¿†ã€å·¥å…·ï¼‰ç»“åˆä½¿ç”¨")
        print()
        print("ä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹ 03-extensions/ å­¦ä¹ æ‰©å±•åŠŸèƒ½")
        print("  2. æŸ¥çœ‹ 04-integration/ å­¦ä¹ é›†æˆæ¡ˆä¾‹")
        print("  3. æŸ¥çœ‹ examples/ ç›®å½•å­¦ä¹ å®é™…åº”ç”¨")
        print("=" * 80 + "\n")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())