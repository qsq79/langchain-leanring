# LangChain Chains å­¦ä¹ æŒ‡å— (LangChain 1.0+ ç‰ˆæœ¬)

**é‡è¦æ›´æ–° (2025)**: LangChain 1.0+ å¼•å…¥äº† `create_agent()` API,è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é«˜çº§æ¥å£,å·²ç»é›†æˆäº†æç¤ºè¯ã€æ¨¡å‹ã€å·¥å…·ã€è®°å¿†å’Œç»“æ„åŒ–è¾“å‡ºã€‚å¯¹äºå¤§å¤šæ•°ä½¿ç”¨åœºæ™¯,**æ¨èä½¿ç”¨ `create_agent()` è€Œä¸æ˜¯æ‰‹åŠ¨æ„å»ºé“¾**ã€‚

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨ç†è§£:
- ä½•æ—¶ä½¿ç”¨ `create_agent()` (æ¨è)
- ä½•æ—¶ä½¿ç”¨ LCEL (LangChain Expression Language)
- ä½•æ—¶ä½¿ç”¨ LangGraph Graph API

## ğŸ“‹ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. ä¸‰ç§ API çš„é€‰æ‹©æŒ‡å—

#### 1.1 `create_agent()` - æ¨è(å¤§å¤šæ•°åœºæ™¯)

**ä½¿ç”¨åœºæ™¯**:
- âœ… éœ€è¦ä½¿ç”¨å·¥å…·(tools)çš„æ™ºèƒ½ä½“
- âœ… éœ€è¦å¯¹è¯è®°å¿†(memory)
- âœ… éœ€è¦ç»“æ„åŒ–è¾“å‡º
- âœ… éœ€è¦è‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œ
- âœ… ç”Ÿäº§ç¯å¢ƒçš„ Agent åº”ç”¨

**ç¤ºä¾‹**:
```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

@tool
def calculator(expression: str) -> str:
    """æ‰§è¡Œæ•°å­¦è®¡ç®—"""
    return str(eval(expression))

# ä½¿ç”¨ create_agent åˆ›å»ºæ™ºèƒ½ä½“
agent = create_agent(
    model="gpt-4o-mini",
    tools=[calculator],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ•°å­¦åŠ©æ‰‹",
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "è®¡ç®— 25 * 4"}]
})
```

#### 1.2 LCEL (LangChain Expression Language) - ç®€å•é“¾

**ä½¿ç”¨åœºæ™¯**:
- âœ… ç®€å•çš„ prompt â†’ model â†’ parser æµç¨‹
- âœ… **ä¸éœ€è¦**ä½¿ç”¨å·¥å…·
- âœ… **ä¸éœ€è¦**å¯¹è¯è®°å¿†
- âœ… å¿«é€ŸåŸå‹å’Œç®€å•ä»»åŠ¡
- âœ… éœ€è¦ç²¾ç»†æ§åˆ¶æ¯ä¸ªæ­¥éª¤

**ç¤ºä¾‹**:
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ä½¿ç”¨ LCEL æ„å»ºç®€å•é“¾
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("è§£é‡Š: {topic}")
chain = prompt | llm | StrOutputParser()

result = chain.invoke({"topic": "é‡å­è®¡ç®—"})
```

#### 1.3 LangGraph Graph API - å¤æ‚å·¥ä½œæµ

**ä½¿ç”¨åœºæ™¯**:
- âœ… å¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµ
- âœ… éœ€è¦æ˜¾å¼çš„çŠ¶æ€ç®¡ç†
- âœ… å¤æ‚çš„æ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯
- âœ… éœ€è¦å¯è§†åŒ–å·¥ä½œæµ
- âœ… è‡ªå®šä¹‰æ‰§è¡Œé€»è¾‘

**ç¤ºä¾‹**:
```python
from langgraph.graph import StateGraph
from typing import TypedDict

class AgentState(TypedDict):
    messages: list
    step_count: int

def call_llm(state):
    # LLM èŠ‚ç‚¹é€»è¾‘
    pass

def should_continue(state):
    # æ¡ä»¶è·¯ç”±é€»è¾‘
    return "continue" if state["step_count"] < 3 else "end"

# ä½¿ç”¨ Graph API æ„å»ºå¤æ‚å·¥ä½œæµ
workflow = StateGraph(AgentState)
workflow.add_node("llm", call_llm)
workflow.add_conditional_edges("llm", should_continue)
app = workflow.compile()
```

### 2. å¯¹æ¯”è¡¨

| ç‰¹æ€§ | create_agent() | LCEL | LangGraph Graph API |
|-----|----------------|------|---------------------|
| **é€‚ç”¨åœºæ™¯** | Agent åº”ç”¨ | ç®€å•é“¾ | å¤æ‚å·¥ä½œæµ |
| **å·¥å…·æ”¯æŒ** | âœ… åŸç”Ÿæ”¯æŒ | âŒ éœ€è¦æ‰‹åŠ¨å®ç° | âœ… åŸç”Ÿæ”¯æŒ |
| **è®°å¿†ç®¡ç†** | âœ… å†…ç½® checkpointer | âŒ éœ€è¦æ‰‹åŠ¨å®ç° | âœ… çŠ¶æ€ç®¡ç† |
| **ç»“æ„åŒ–è¾“å‡º** | âœ… response_format | âœ… with_structured_output() | âœ… è‡ªå®šä¹‰ |
| **å­¦ä¹ æ›²çº¿** | ä½ | ä½ | ä¸­-é«˜ |
| **ä»£ç é‡** | æœ€å°‘ | å°‘ | å¤š |
| **çµæ´»æ€§** | ä¸­ | ä¸­ | é«˜ |
| **ç”Ÿäº§å°±ç»ª** | âœ… | âœ… | âœ… |

## ğŸ¯ å¸¸è§é¢è¯•é¢˜

### åŸºç¡€æ¦‚å¿µé¢˜

**Q1: LangChain 1.0+ ä¸­åº”è¯¥ä½¿ç”¨ `create_agent()` è¿˜æ˜¯ LCEL?**

**A1:**
- **ä½¿ç”¨ `create_agent()`** å½“ä½ éœ€è¦:
  - ä½¿ç”¨å·¥å…·(tools)è®© LLM æ‰§è¡Œæ“ä½œ
  - å¯¹è¯è®°å¿†å’ŒçŠ¶æ€ç®¡ç†
  - ç»“æ„åŒ–è¾“å‡º
  - Agent è‡ªä¸»è§„åˆ’èƒ½åŠ›

- **ä½¿ç”¨ LCEL** å½“ä½ éœ€è¦:
  - ç®€å•çš„ prompt â†’ model æµç¨‹
  - ä¸éœ€è¦å·¥å…·å’Œè®°å¿†
  - æ›´ç²¾ç»†çš„æ­¥éª¤æ§åˆ¶
  - å¿«é€ŸåŸå‹éªŒè¯

- **ä½¿ç”¨ LangGraph Graph API** å½“ä½ éœ€è¦:
  - å¤æ‚çš„å¤šæ­¥éª¤å·¥ä½œæµ
  - æ˜¾å¼çš„çŠ¶æ€ç®¡ç†å’Œå¯è§†åŒ–
  - è‡ªå®šä¹‰çš„æ‰§è¡Œé€»è¾‘å’Œé”™è¯¯å¤„ç†

**Q2: `create_agent()` ç›¸æ¯”æ‰‹åŠ¨æ„å»ºé“¾æœ‰ä»€ä¹ˆä¼˜åŠ¿?**

**A2:**
- **ç»Ÿä¸€æ¥å£**: ä¸€ä¸ªå‡½æ•°å¤„ç†æ‰€æœ‰ Agent ç›¸å…³é…ç½®
- **å†…ç½®åŠŸèƒ½**: è‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨ã€è®°å¿†ç®¡ç†ã€æµå¼è¾“å‡º
- **ç”Ÿäº§å°±ç»ª**: åŸºäºç¨³å®šçš„ LangGraph è¿è¡Œæ—¶
- **æ›´å°‘ä»£ç **: ä¸éœ€è¦æ‰‹åŠ¨ç»„åˆ promptã€modelã€parser
- **ç±»å‹å®‰å…¨**: æ”¯æŒ Pydantic v2 çš„ç»“æ„åŒ–è¾“å‡º
- **æ˜“äºæ‰©å±•**: æ”¯æŒä¸­é—´ä»¶ã€å­ Agentã€äººå·¥å¹²é¢„

### æŠ€æœ¯å®ç°é¢˜

**Q3: å¦‚ä½•å®ç°ä¸€ä¸ªç®€å•çš„é—®ç­”åŠ©æ‰‹?**

**A3: ä½¿ç”¨ `create_agent()` (æ¨è)**
```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

qa_agent = create_agent(
    model="gpt-4o-mini",
    tools=[],  # ä¸éœ€è¦å·¥å…·
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚",
)

response = qa_agent.invoke({
    "messages": [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ LangChain?"}]
})
print(response["messages"][-1].content)
```

**æˆ–è€…ä½¿ç”¨ LCEL (æ›´è½»é‡)**
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚\n\né—®é¢˜: {question}"
)
qa_chain = prompt | llm | StrOutputParser()

response = qa_chain.invoke({"question": "ä»€ä¹ˆæ˜¯ LangChain?"})
print(response)
```

**Q4: å¦‚ä½•å®ç°ä¸€ä¸ªå¸¦è®°å¿†çš„å¯¹è¯ Agent?**

**A4:**
```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver

# åˆ›å»ºå¸¦è®°å¿†çš„ Agent
memory = MemorySaver()

chat_agent = create_agent(
    model="gpt-4o-mini",
    tools=[],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„èŠå¤©åŠ©æ‰‹",
    checkpointer=memory,  # æ·»åŠ è®°å¿†
)

# ä½¿ç”¨ thread_id ä¿æŒä¼šè¯
config = {"configurable": {"thread_id": "user-123"}}

# ç¬¬ä¸€è½®å¯¹è¯
response1 = chat_agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«å¼ ä¸‰"}]},
    config
)

# ç¬¬äºŒè½®å¯¹è¯ - Agent è®°ä½äº†ä¹‹å‰çš„å¯¹è¯
response2 = chat_agent.invoke(
    {"messages": [{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—?"}]},
    config
)
print(response2["messages"][-1].content)  # è¾“å‡º: ä½ å«å¼ ä¸‰
```

**Q5: å¦‚ä½•å®ç°ç»“æ„åŒ–è¾“å‡º?**

**A5: ä½¿ç”¨ `create_agent()` çš„ `response_format`**
```python
from pydantic import BaseModel, Field
from typing import List
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

class AnalysisResult(BaseModel):
    """åˆ†æç»“æœçš„ç»“æ„åŒ–è¾“å‡º"""
    summary: str = Field(description="é—®é¢˜æ€»ç»“")
    key_points: List[str] = Field(description="å…³é”®ç‚¹åˆ—è¡¨")
    confidence: float = Field(description="ç½®ä¿¡åº¦ (0-1)")

analysis_agent = create_agent(
    model="gpt-4o-mini",
    tools=[],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹",
    response_format=AnalysisResult,  # ç»“æ„åŒ–è¾“å‡º
)

response = analysis_agent.invoke({
    "messages": [{"role": "user", "content": "åˆ†æè¿™æ®µæ–‡æœ¬: ..."}]
})

# è®¿é—®ç»“æ„åŒ–è¾“å‡º
result = response.structuredResponse
print(f"æ€»ç»“: {result.summary}")
print(f"å…³é”®ç‚¹: {result.key_points}")
print(f"ç½®ä¿¡åº¦: {result.confidence}")
```

## ğŸ—ï¸ è¿ç§»æŒ‡å—

### ä»æ—§å¼ Chain è¿ç§»åˆ° `create_agent()`

#### âŒ æ—§æ–¹å¼ (å·²å¼ƒç”¨)
```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    template="å›ç­”: {question}",
    input_variables=["question"]
)
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(question="ä»€ä¹ˆæ˜¯ AI?")
```

#### âœ… æ–°æ–¹å¼ (æ¨è)
```python
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o-mini",
    tools=[],
    system_prompt="å›ç­”ç”¨æˆ·çš„é—®é¢˜",
)
result = agent.invoke({
    "messages": [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ AI?"}]
})
```

### ä» LCEL è¿ç§»åˆ° `create_agent()`

#### ä½¿ç”¨ LCEL (ä»ç„¶æœ‰æ•ˆ)
```python
chain = prompt | llm | StrOutputParser()
result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯ AI?"})
```

#### è¿ç§»åˆ° `create_agent()` (å¦‚æœéœ€è¦æ›´å¤šåŠŸèƒ½)
```python
agent = create_agent(
    model="gpt-4o-mini",
    tools=[],
    system_prompt=prompt.template,
)
result = agent.invoke({
    "messages": [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ AI?"}]
})
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©æ­£ç¡®çš„ API

```python
# âœ… æ­£ç¡®: ä½¿ç”¨ create_agent() æ„å»º Agent
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o-mini",
    tools=[search_tool, calculator],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©ç†",
)

# âœ… æ­£ç¡®: ä½¿ç”¨ LCEL æ„å»ºç®€å•é“¾
from langchain_core.output_parsers import StrOutputParser

simple_chain = prompt | llm | StrOutputParser()

# âœ… æ­£ç¡®: ä½¿ç”¨ Graph API æ„å»ºå¤æ‚å·¥ä½œæµ
from langgraph.graph import StateGraph

workflow = StateGraph(AgentState)
# ... æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
```

### 2. ä½¿ç”¨ Pydantic v2 å®šä¹‰ç»“æ„åŒ–è¾“å‡º

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class TaskResult(BaseModel):
    """ä»»åŠ¡ç»“æœçš„ç»“æ„åŒ–è¾“å‡º"""
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="çŠ¶æ€: success/failed/pending")
    result: Optional[str] = Field(default=None, description="æ‰§è¡Œç»“æœ")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

agent = create_agent(
    model="gpt-4o-mini",
    tools=[task_executor],
    system_prompt="æ‰§è¡Œä»»åŠ¡å¹¶è¿”å›ç»“æ„åŒ–ç»“æœ",
    response_format=TaskResult,
)
```

### 3. æ·»åŠ è®°å¿†æ”¯æŒ

```python
from langgraph.checkpoint.memory import MemorySaver

# å†…å­˜å­˜å‚¨ (å¼€å‘ç¯å¢ƒ)
memory = MemorySaver()

# ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨
# from langgraph.checkpoint.postgres import PostgresSaver
# memory = PostgresSaver.from_conn_string("postgresql://...")

agent = create_agent(
    model="gpt-4o-mini",
    tools=[],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰è®°å¿†çš„åŠ©æ‰‹",
    checkpointer=memory,
)

# ä½¿ç”¨ thread_id ä¿æŒä¼šè¯
config = {"configurable": {"thread_id": "session-123"}}
response = agent.invoke(
    {"messages": [{"role": "user", "content": "ä½ å¥½"}]},
    config
)
```

### 4. ä½¿ç”¨ä¸­é—´ä»¶åŠ¨æ€ä¿®æ”¹æç¤º

```python
from langchain_core.middleware import dynamic_prompt

@dynamic_prompt
def add_context(request):
    """æ ¹æ®è¯·æ±‚åŠ¨æ€æ·»åŠ ä¸Šä¸‹æ–‡"""
    user_id = request.config.get("context", {}).get("user_id")
    if user_id == "premium":
        return "\n\nè¿™æ˜¯ä¸€ä¸ªé«˜çº§ç”¨æˆ·,æä¾›æ›´è¯¦ç»†çš„æœåŠ¡ã€‚"
    return "\n\nè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†ç”¨æˆ·,æä¾›ç®€æ´çš„æœåŠ¡ã€‚"

agent = create_agent(
    model="gpt-4o-mini",
    tools=[],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹",  # åŸºç¡€æç¤º
    middleware=[add_context],  # åŠ¨æ€ä¿®æ”¹
)

# é«˜çº§ç”¨æˆ·è·å¾—æ›´è¯¦ç»†çš„æœåŠ¡
response = agent.invoke(
    {"messages": [{"role": "user", "content": "è§£é‡Šé‡å­è®¡ç®—"}]},
    {"config": {"context": {"user_id": "premium"}}}
)
```

## ğŸ“š ç›¸å…³èµ„æº

- [LangChain Agents å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/agents)
- [LangGraph Graph API æ–‡æ¡£](https://docs.langchain.com/oss/python/langgraph)
- [LCEL æŒ‡å—](https://python.langchain.com/docs/expression_language/)
- [è¿ç§»æŒ‡å—](https://docs.langchain.com/oss/python/migrate/langchain-v1)

---

ğŸ’¡ **å­¦ä¹ å»ºè®®**:
1. **ä¼˜å…ˆå­¦ä¹  `create_agent()`** - è¿™æ˜¯å¤§å¤šæ•°åº”ç”¨çš„æ¨èæ–¹å¼
2. **æŒæ¡ LCEL** - ç”¨äºæ„å»ºç®€å•é“¾å’Œå¿«é€ŸåŸå‹
3. **äº†è§£ Graph API** - å½“éœ€è¦æ„å»ºå¤æ‚å·¥ä½œæµæ—¶ä½¿ç”¨
4. **æŸ¥çœ‹ç¤ºä¾‹** - å‚è€ƒ 06-agents å’Œ 03-chains ç›®å½•ä¸‹çš„ç¤ºä¾‹ä»£ç 
