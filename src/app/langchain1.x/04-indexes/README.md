# LangChain Indexes ç»„ä»¶å­¦ä¹ æŒ‡å—

Indexesæ˜¯LangChainæ¡†æ¶ä¸­ç”¨äºå¤„ç†æ–‡æ¡£ã€æ„å»ºç´¢å¼•å’Œå®ç°æ£€ç´¢çš„æ ¸å¿ƒç»„ä»¶ã€‚æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»Indexesç»„ä»¶çš„æ ¸å¿ƒæ¦‚å¿µã€ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‹ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. Document Loadersï¼ˆæ–‡æ¡£åŠ è½½å™¨ï¼‰

#### 1.1 åŸºç¡€æ–‡æ¡£åŠ è½½å™¨
- **TextLoader**ï¼šåŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶
- **CSVLoader**ï¼šåŠ è½½CSVæ ¼å¼æ•°æ®
- **JSONLoader**ï¼šåŠ è½½JSONæ ¼å¼æ•°æ®
- **UnstructuredLoader**ï¼šåŠ è½½å„ç§éç»“æ„åŒ–æ–‡æ¡£ï¼ˆPDFã€Wordã€HTMLç­‰ï¼‰

#### 1.2 ç½‘ç»œæ•°æ®åŠ è½½å™¨
- **WebBaseLoader**ï¼šåŠ è½½ç½‘é¡µå†…å®¹
- **ArxivLoader**ï¼šåŠ è½½å­¦æœ¯è®ºæ–‡
- **WikipediaLoader**ï¼šåŠ è½½ç»´åŸºç™¾ç§‘å†…å®¹
- **GitHubLoader**ï¼šåŠ è½½GitHubä»“åº“å†…å®¹

#### 1.3 æ•°æ®åº“è¿æ¥å™¨
- **SQLDatabaseLoader**ï¼šåŠ è½½SQLæ•°æ®åº“æ•°æ®
- **MongoDBLoader**ï¼šåŠ è½½MongoDBæ•°æ®
- **ChromaLoader**ï¼šåŠ è½½Chromaå‘é‡æ•°æ®åº“

### 2. Text Splittersï¼ˆæ–‡æœ¬åˆ†å‰²å™¨ï¼‰

#### 2.1 åŸºç¡€åˆ†å‰²ç­–ç•¥
- **CharacterTextSplitter**ï¼šæŒ‰å­—ç¬¦åˆ†å‰²
- **RecursiveCharacterTextSplitter**ï¼šé€’å½’å­—ç¬¦åˆ†å‰²
- **TokenTextSplitter**ï¼šæŒ‰Tokenåˆ†å‰²
- **MarkdownTextSplitter**ï¼šæŒ‰Markdownç»“æ„åˆ†å‰²

#### 2.2 è¯­ä¹‰åˆ†å‰²
- **SemanticTextSplitter**ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å‰²
- **NLTKTextSplitter**ï¼šåŸºäºè‡ªç„¶è¯­è¨€å¤„ç†åˆ†å‰²
- **SpacyTextSplitter**ï¼šåŸºäºSpacy NLPåº“åˆ†å‰²

#### 2.3 åˆ†å‰²å‚æ•°é…ç½®
- **chunk_size**ï¼šåˆ†å‰²å—å¤§å°
- **chunk_overlap**ï¼šé‡å éƒ¨åˆ†å¤§å°
- **separators**ï¼šåˆ†éš”ç¬¦åˆ—è¡¨
- **length_function**ï¼šé•¿åº¦è®¡ç®—å‡½æ•°

### 3. Vector Storesï¼ˆå‘é‡å­˜å‚¨ï¼‰

#### 3.1 å†…å­˜å‘é‡å­˜å‚¨
- **FAISS**ï¼šFacebookå¼€å‘çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢åº“
- **Chroma**ï¼šå¼€æºçš„å‘é‡æ•°æ®åº“
- **InMemoryVectorStore**ï¼šç®€å•çš„å†…å­˜å‘é‡å­˜å‚¨

#### 3.2 äº‘ç«¯å‘é‡æ•°æ®åº“
- **Pinecone**ï¼šæ‰˜ç®¡å‘é‡æ•°æ®åº“æœåŠ¡
- **Weaviate**ï¼šå¼€æºå‘é‡æœç´¢å¼•æ“
- **Qdrant**ï¼šé«˜æ€§èƒ½å‘é‡ç›¸ä¼¼åº¦æœç´¢å¼•æ“

#### 3.3 ä¼ ç»Ÿæ•°æ®åº“é›†æˆ
- **PostgreSQL + pgvector**ï¼šPostgreSQLçš„å‘é‡æ‰©å±•
- **Redis + RediSearch**ï¼šRedisçš„å‘é‡æœç´¢åŠŸèƒ½
- **Elasticsearch**ï¼šæ”¯æŒå‘é‡æœç´¢çš„æœç´¢å¼•æ“

### 4. Retrieversï¼ˆæ£€ç´¢å™¨ï¼‰

#### 4.1 åŸºç¡€æ£€ç´¢å™¨
- **VectorStoreRetriever**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢
- **MultiQueryRetriever**ï¼šå¤šæŸ¥è¯¢æ£€ç´¢
- **ContextualCompressionRetriever**ï¼šä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢

#### 4.2 æ··åˆæ£€ç´¢ç­–ç•¥
- **EnsembleRetriever**ï¼šé›†æˆå¤šç§æ£€ç´¢ç­–ç•¥
- **ParentDocumentRetriever**ï¼šçˆ¶å­æ–‡æ¡£æ£€ç´¢
- **SelfQueryRetriever**ï¼šè‡ªæŸ¥è¯¢æ£€ç´¢

## ğŸ¯ å¸¸è§é¢è¯•é¢˜

### åŸºç¡€æ¦‚å¿µé¢˜

**Q1: ä»€ä¹ˆæ˜¯LangChainä¸­çš„Indexç»„ä»¶ï¼Œå®ƒçš„ä¸»è¦ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**

**A1:**
- **å®šä¹‰**ï¼šIndexæ˜¯LangChainä¸­ç”¨äºç»“æ„åŒ–éç»“æ„åŒ–æ•°æ®ã€æ„å»ºå¯æ£€ç´¢çŸ¥è¯†åº“çš„ç»„ä»¶é›†åˆ
- **ä¸»è¦ä½œç”¨**ï¼š
  - **æ•°æ®é¢„å¤„ç†**ï¼šå°†å„ç§æ ¼å¼çš„æ–‡æ¡£è½¬æ¢ä¸ºç»Ÿä¸€ç»“æ„
  - **æ–‡æœ¬åˆ†å‰²**ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„å—
  - **å‘é‡åŒ–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡è¡¨ç¤º
  - **ç´¢å¼•æ„å»º**ï¼šæ„å»ºé«˜æ•ˆçš„æ£€ç´¢ç´¢å¼•ç»“æ„
  - **ç›¸ä¼¼åº¦æ£€ç´¢**ï¼šåŸºäºæŸ¥è¯¢å¿«é€Ÿæ‰¾åˆ°ç›¸å…³æ–‡æ¡£
- **æ ¸å¿ƒä»·å€¼**ï¼šå®ç°åŸºäºçŸ¥è¯†åº“çš„é—®ç­”ç³»ç»Ÿï¼Œæä¾›ä¸Šä¸‹æ–‡ç›¸å…³çš„å›ç­”

**Q2: Text Splitteråœ¨æ–‡æ¡£å¤„ç†ä¸­çš„é‡è¦æ€§æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•é€‰æ‹©åˆé€‚çš„åˆ†å‰²ç­–ç•¥ï¼Ÿ**

**A2:**
- **é‡è¦æ€§**ï¼š
  - **ä¸Šä¸‹æ–‡å®Œæ•´æ€§**ï¼šç¡®ä¿æ¯ä¸ªåˆ†å‰²å—åŒ…å«å®Œæ•´è¯­ä¹‰ä¿¡æ¯
  - **æ¨¡å‹å…¼å®¹æ€§**ï¼šé€‚åº”LLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
  - **æ£€ç´¢ç²¾åº¦**ï¼šæé«˜æ–‡æ¡£æ£€ç´¢çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§
  - **å¤„ç†æ•ˆç‡**ï¼šå¹³è¡¡æ£€ç´¢ç²¾åº¦å’Œè®¡ç®—æˆæœ¬

- **é€‰æ‹©ç­–ç•¥**ï¼š
  - **æ–‡æ¡£ç±»å‹**ï¼šæŠ€æœ¯æ–‡æ¡£ç”¨é€’å½’åˆ†å‰²ï¼Œæ³•å¾‹æ–‡æ¡£ç”¨è¯­ä¹‰åˆ†å‰²
  - **å†…å®¹é•¿åº¦**ï¼šé•¿æ–‡æ¡£ç”¨å¤§å—åˆ†å‰²ï¼ŒçŸ­æ–‡æ¡£ç”¨å°å—åˆ†å‰²
  - **æ£€ç´¢éœ€æ±‚**ï¼šç²¾ç¡®åŒ¹é…ç”¨å°é‡å ï¼Œè¯­ä¹‰æœç´¢ç”¨å¤§é‡å 
  - **æ€§èƒ½è€ƒè™‘**ï¼šå†…å­˜é™åˆ¶ç”¨å°å—ï¼Œæ£€ç´¢é€Ÿåº¦ç”¨å¤§å—

### æŠ€æœ¯å®ç°é¢˜

**Q3: å¦‚ä½•å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„Document Loaderï¼Ÿ**

**A3:**
```python
from langchain_core.documents import Document
from langchain_community.document_loaders.base import BaseLoader
from typing import List, Optional, Iterator

class CustomDocumentLoader(BaseLoader):
    """è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹"""
    
    def __init__(self, file_path: str, encoding: str = 'utf-8'):
        self.file_path = file_path
        self.encoding = encoding
    
    def load(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        with open(self.file_path, 'r', encoding=self.encoding) as file:
            content = file.read()
        
        # è‡ªå®šä¹‰è§£æé€»è¾‘
        documents = self._parse_content(content)
        return documents
    
    def _parse_content(self, content: str) -> List[Document]:
        """è§£æå†…å®¹ä¸ºDocumentå¯¹è±¡"""
        documents = []
        
        # æŒ‰è¡Œåˆ†å‰²å†…å®¹
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            if line.strip():  # è·³è¿‡ç©ºè¡Œ
                doc = Document(
                    page_content=line.strip(),
                    metadata={
                        "source": self.file_path,
                        "line_number": i + 1,
                        "encoding": self.encoding
                    }
                )
                documents.append(doc)
        
        return documents
    
    def lazy_load(self) -> Iterator[Document]:
        """æ‡’åŠ è½½æ–‡æ¡£"""
        with open(self.file_path, 'r', encoding=self.encoding) as file:
            for i, line in enumerate(file):
                if line.strip():
                    yield Document(
                        page_content=line.strip(),
                        metadata={
                            "source": self.file_path,
                            "line_number": i + 1
                        }
                    )
```

**Q4: å¦‚ä½•å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„Text Splitterï¼Ÿ**

**A4:**
```python
from langchain_core.text_splitter import TextSplitter
from langchain_core.documents import Document
from typing import List
import re

class CustomSemanticSplitter(TextSplitter):
    """åŸºäºè¯­ä¹‰çš„è‡ªå®šä¹‰æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        semantic_threshold: float = 0.3
    ):
        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.semantic_threshold = semantic_threshold
    
    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰å¥å­åˆ†å‰²
        sentences = self._split_into_sentences(text)
        
        # è®¡ç®—å¥å­é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        sentence_groups = self._group_sentences_by_semantics(sentences)
        
        # åˆå¹¶ä¸ºæœ€ç»ˆçš„åˆ†å—
        chunks = self._merge_groups_to_chunks(sentence_groups)
        
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ]'
        sentences = re.split(sentence_endings, text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _group_sentences_by_semantics(self, sentences: List[str]) -> List[List[str]]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†ç»„å¥å­"""
        groups = []
        current_group = [sentences[0]]
        
        for i in range(1, len(sentences)):
            similarity = self._calculate_semantic_similarity(
                sentences[i-1], sentences[i]
            )
            
            if similarity > self.semantic_threshold:
                current_group.append(sentences[i])
            else:
                groups.append(current_group)
                current_group = [sentences[i]]
        
        groups.append(current_group)
        return groups
    
    def _calculate_semantic_similarity(self, sent1: str, sent2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨å®é™…çš„åµŒå…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
        # ç®€åŒ–ç¤ºä¾‹ï¼šåŸºäºè¯æ±‡é‡å è®¡ç®—ç›¸ä¼¼åº¦
        words1 = set(sent1.lower().split())
        words2 = set(sent2.lower().split())
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if len(union) == 0:
            return 0.0
        
        return len(intersection) / len(union)
    
    def _merge_groups_to_chunks(self, groups: List[List[str]]) -> List[str]:
        """å°†è¯­ä¹‰ç»„åˆå¹¶ä¸ºæœ€ç»ˆåˆ†å—"""
        chunks = []
        current_chunk = ""
        
        for group in groups:
            group_text = " ".join(group)
            
            if len(current_chunk) + len(group_text) <= self.chunk_size:
                current_chunk += " " + group_text
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = group_text
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        split_docs = []
        
        for doc in documents:
            chunks = self.split_text(doc.page_content)
            
            for i, chunk in enumerate(chunks):
                new_doc = Document(
                    page_content=chunk,
                    metadata={
                        **doc.metadata,
                        "chunk_index": i,
                        "source_type": "custom_semantic_split"
                    }
                )
                split_docs.append(new_doc)
        
        return split_docs
```

### æ¶æ„è®¾è®¡é¢˜

**Q5: LangChainçš„Indexç»„ä»¶é‡‡ç”¨äº†ä»€ä¹ˆè®¾è®¡æ¨¡å¼ï¼Ÿ**

**A5:**
- **ç­–ç•¥æ¨¡å¼**ï¼šä¸åŒçš„Text Splitterå®ç°ä¸åŒçš„åˆ†å‰²ç­–ç•¥
- **å·¥å‚æ¨¡å¼**ï¼šDocument Loaderçš„åˆ›å»ºå’Œä½¿ç”¨
- **é€‚é…å™¨æ¨¡å¼**ï¼šVector Storeé€‚é…ä¸åŒçš„æ•°æ®åº“åç«¯
- **è£…é¥°å™¨æ¨¡å¼**ï¼šRetrieverå¯¹åŸºç¡€å­˜å‚¨çš„å¢å¼º
- **ç»„åˆæ¨¡å¼**ï¼šå°†å¤šä¸ªç»„ä»¶ç»„åˆæˆå®Œæ•´çš„ç´¢å¼•ç³»ç»Ÿ
- **è¿­ä»£å™¨æ¨¡å¼**ï¼šDocument Loaderçš„æ‡’åŠ è½½æœºåˆ¶

## ğŸ—ï¸ è®¾è®¡æ€è·¯å’Œè®¾è®¡æ¨¡å¼

### 1. æ•°æ®æµè®¾è®¡

#### 1.1 å¤„ç†ç®¡é“
```python
class IndexPipeline:
    """ç´¢å¼•å¤„ç†ç®¡é“"""
    
    def __init__(self, loader, splitter, embedder, vector_store):
        self.loader = loader
        self.splitter = splitter
        self.embedder = embedder
        self.vector_store = vector_store
    
    def process(self, source):
        # 1. åŠ è½½æ–‡æ¡£
        documents = self.loader.load(source)
        
        # 2. åˆ†å‰²æ–‡æ¡£
        chunks = self.splitter.split_documents(documents)
        
        # 3. ç”ŸæˆåµŒå…¥
        embeddings = self.embedder.embed_documents([chunk.page_content for chunk in chunks])
        
        # 4. å­˜å‚¨å‘é‡
        self.vector_store.add_texts([chunk.page_content for chunk in chunks], embeddings, 
                                  [chunk.metadata for chunk in chunks])
        
        return len(chunks)
```

#### 1.2 é”™è¯¯å¤„ç†è®¾è®¡
```python
class ResilientIndexer:
    """å…·æœ‰å®¹é”™èƒ½åŠ›çš„ç´¢å¼•å™¨"""
    
    def __init__(self, max_retries=3, fallback_strategy="skip"):
        self.max_retries = max_retries
        self.fallback_strategy = fallback_strategy
    
    def safe_process_document(self, document):
        """å®‰å…¨å¤„ç†æ–‡æ¡£"""
        for attempt in range(self.max_retries):
            try:
                return self.process_document(document)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    if self.fallback_strategy == "skip":
                        logger.warning(f"è·³è¿‡æ–‡æ¡£: {e}")
                        return None
                    else:
                        raise e
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### 2. æ€§èƒ½ä¼˜åŒ–è®¾è®¡

#### 2.1 æ‰¹å¤„ç†ä¼˜åŒ–
```python
class BatchProcessor:
    """æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, batch_size=100):
        self.batch_size = batch_size
    
    def process_in_batches(self, documents, process_func):
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        results = []
        
        for i in range(0, len(documents), self.batch_size):
            batch = documents[i:i + self.batch_size]
            batch_results = process_func(batch)
            results.extend(batch_results)
        
        return results
```

#### 2.2 ç¼“å­˜æœºåˆ¶
```python
class CachedEmbedder:
    """å¸¦ç¼“å­˜çš„åµŒå…¥å™¨"""
    
    def __init__(self, base_embedder, cache_size=10000):
        self.base_embedder = base_embedder
        self.cache = {}
        self.cache_size = cache_size
    
    def embed_documents(self, texts):
        """åµŒå…¥æ–‡æ¡£ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        embeddings = []
        cache_hits = 0
        
        for text in texts:
            text_hash = hash(text)
            
            if text_hash in self.cache:
                embedding = self.cache[text_hash]
                cache_hits += 1
            else:
                embedding = self.base_embedder.embed_query(text)
                
                if len(self.cache) < self.cache_size:
                    self.cache[text_hash] = embedding
            
            embeddings.append(embedding)
        
        return embeddings
```

### 3. å¯æ‰©å±•æ€§è®¾è®¡

#### 3.1 æ’ä»¶åŒ–æ¶æ„
```python
class PluginManager:
    """æ’ä»¶ç®¡ç†å™¨"""
    
    def __init__(self):
        self.loaders = {}
        self.splitters = {}
        self.vector_stores = {}
    
    def register_loader(self, name, loader_class):
        """æ³¨å†Œæ–‡æ¡£åŠ è½½å™¨"""
        self.loaders[name] = loader_class
    
    def register_splitter(self, name, splitter_class):
        """æ³¨å†Œæ–‡æœ¬åˆ†å‰²å™¨"""
        self.splitters[name] = splitter_class
    
    def register_vector_store(self, name, store_class):
        """æ³¨å†Œå‘é‡å­˜å‚¨"""
        self.vector_stores[name] = store_class
    
    def create_pipeline(self, config):
        """æ ¹æ®é…ç½®åˆ›å»ºå¤„ç†ç®¡é“"""
        loader = self.loaders[config["loader"]]()
        splitter = self.splitters[config["splitter"]]()
        vector_store = self.vector_stores[config["vector_store"]]()
        
        return IndexPipeline(loader, splitter, vector_store)
```

#### 3.2 é…ç½®é©±åŠ¨è®¾è®¡
```python
class ConfigurableIndexer:
    """å¯é…ç½®çš„ç´¢å¼•å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.components = self._build_components()
    
    def _build_components(self):
        """æ ¹æ®é…ç½®æ„å»ºç»„ä»¶"""
        components = {}
        
        # æ„å»ºæ–‡æ¡£åŠ è½½å™¨
        if "loaders" in self.config:
            components["loaders"] = {}
            for name, loader_config in self.config["loaders"].items():
                components["loaders"][name] = self._create_loader(loader_config)
        
        # æ„å»ºæ–‡æœ¬åˆ†å‰²å™¨
        if "splitters" in self.config:
            components["splitters"] = {}
            for name, splitter_config in self.config["splitters"].items():
                components["splitters"][name] = self._create_splitter(splitter_config)
        
        return components
```

## ğŸš€ æœ€ä½³å®è·µ

### 1. æ–‡æ¡£å¤„ç†ç­–ç•¥

1. **é¢„å¤„ç†ä¼˜åŒ–**ï¼š
   - æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
   - æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼å’Œç¼–ç 
   - ç§»é™¤é‡å¤å†…å®¹å’Œå™ªéŸ³

2. **åˆ†å‰²ç­–ç•¥é€‰æ‹©**ï¼š
   - æŠ€æœ¯æ–‡æ¡£ï¼šæŒ‰ç« èŠ‚å’Œæ®µè½åˆ†å‰²
   - æ³•å¾‹æ–‡æ¡£ï¼šä¿æŒæ¡æ¬¾å®Œæ•´æ€§
   - å¯¹è¯å†…å®¹ï¼šæŒ‰è¯´è¯è€…åˆ†å‰²

3. **å…ƒæ•°æ®ç®¡ç†**ï¼š
   - ä¿å­˜åŸå§‹æ¥æºä¿¡æ¯
   - æ·»åŠ åˆ†å‰²ä½ç½®æ ‡è®°
   - åŒ…å«æ–‡æ¡£ç±»å‹å’Œåˆ†ç±»ä¿¡æ¯

### 2. æ€§èƒ½ä¼˜åŒ–

```python
# å¼‚æ­¥å¤„ç†
import asyncio

class AsyncIndexer:
    async def process_documents(self, documents):
        """å¼‚æ­¥å¤„ç†æ–‡æ¡£"""
        tasks = []
        
        for doc in documents:
            task = self.process_single_document(doc)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results

# å¹¶è¡Œå‘é‡åŒ–
class ParallelEmbedder:
    def embed_documents_parallel(self, texts, num_workers=4):
        """å¹¶è¡ŒåµŒå…¥æ–‡æ¡£"""
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(self.embed_text, text) for text in texts]
            embeddings = [future.result() for future in futures]
        
        return embeddings
```

### 3. è´¨é‡æ§åˆ¶

```python
class QualityController:
    def validate_document(self, document):
        """éªŒè¯æ–‡æ¡£è´¨é‡"""
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(document.page_content) < 10:
            return False, "å†…å®¹è¿‡çŸ­"
        
        # æ£€æŸ¥é‡å¤å†…å®¹
        if self._is_duplicate(document):
            return False, "é‡å¤å†…å®¹"
        
        # æ£€æŸ¥è¯­è¨€è´¨é‡
        if not self._has_valid_language(document):
            return False, "è¯­è¨€è´¨é‡ä¸ä½³"
        
        return True, "éªŒè¯é€šè¿‡"
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| ç»„ä»¶ç±»å‹ | å¤„ç†é€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | å‡†ç¡®æ€§ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|--------|----------|
| CharacterTextSplitter | å¿« | ä½ | ä½ | ç®€å•æ–‡æœ¬åˆ†å‰² |
| RecursiveCharacterSplitter | ä¸­ | ä¸­ | ä¸­ | é€šç”¨æ–‡æ¡£å¤„ç† |
| SemanticTextSplitter | æ…¢ | é«˜ | é«˜ | è¯­ä¹‰ç›¸å…³åˆ†å‰² |
| FAISS | å¿« | ä¸­ | é«˜ | å¤§è§„æ¨¡å‘é‡æœç´¢ |
| Chroma | ä¸­ | ä¸­ | ä¸­ | ä¸­å°è§„æ¨¡åº”ç”¨ |

## ğŸ”— ç›¸å…³èµ„æº

- [LangChain Indexeså®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/)
- [å‘é‡æ•°æ®åº“æ¯”è¾ƒ](https://zilliz.com/comparison)
- [æ–‡æ¡£å¤„ç†æœ€ä½³å®è·µ](https://python.langchain.com/docs/modules/data_connection/document_transformers/)

---

ğŸ’¡ **å­¦ä¹ å»ºè®®**ï¼šå»ºè®®ä»åŸºç¡€çš„Document Loaderå¼€å§‹å­¦ä¹ ï¼Œç„¶åæŒæ¡Text Splitterçš„ä½¿ç”¨ï¼Œæœ€åå­¦ä¹ Vector Storeå’ŒRetrieverçš„é…ç½®å’Œä¼˜åŒ–ã€‚