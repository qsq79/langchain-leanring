# LangChain 1.x æ ¸å¿ƒçŸ¥è¯†ç‚¹

æœ¬æ–‡æ¡£æ€»ç»“äº† LangChain 1.x çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹å’Œæœ€ä½³å®è·µï¼ŒåŸºäºå®é™…ä»£ç ä¿®æ­£ç»éªŒç¼–å†™ã€‚

## ğŸ“‹ æ ¸å¿ƒæ¶æ„å˜åŒ–

### 1. æ¨¡å—åŒ–æ¶æ„

LangChain 1.x é‡‡ç”¨äº†å…¨æ–°çš„æ¨¡å—åŒ–æ¶æ„ï¼Œå°†åŠŸèƒ½åˆ†æ•£åˆ°ä¸“é—¨çš„åŒ…ä¸­ï¼š

```python
# æ ¸å¿ƒæ¥å£å’ŒåŸºç¡€ç»„ä»¶
from langchain_core import (
    prompts, messages, runnables, callbacks,
    outputs, exceptions
)

# OpenAI ç‰¹å®šé›†æˆ
from langchain_openai import OpenAI, ChatOpenAI, OpenAIEmbeddings

# ç¤¾åŒºè´¡çŒ®çš„ç»„ä»¶
from langchain_community import (
    document_loaders, vectorstores, tools, utilities
)

# æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆç‹¬ç«‹åŒ…ï¼‰
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter
)

# ä¼ ç»Ÿç»„ä»¶çš„å‘åå…¼å®¹
from langchain_classic import (
    memory, chains, agents, tools
)
```

### 2. LangChain Expression Language (LCEL)

LCEL æ˜¯ LangChain 1.x çš„æ ¸å¿ƒåˆ›æ–°ï¼Œä½¿ç”¨ pipe operator (`|`) æ„å»ºå¤„ç†é“¾ï¼š

```python
# åŸºç¡€é“¾æ„å»º
chain = prompt | llm | output_parser

# å¤æ‚é“¾ç»„åˆ
retrieval_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | output_parser
)

# å¹¶è¡Œå¤„ç†
parallel_chain = RunnableParallel({
    "summary": summary_prompt | llm,
    "analysis": analysis_prompt | llm
})
```

## ğŸ—ï¸ ç»„ä»¶è¯¦è§£

### Models ç»„ä»¶

#### LLM vs Chat Model
```python
# LLM - æ–‡æœ¬è¾“å…¥è¾“å‡º
llm = OpenAI(model="gpt-3.5-turbo-instruct")
result = llm.invoke("What is AI?")

# Chat Model - æ¶ˆæ¯è¾“å…¥è¾“å‡º
chat_model = ChatOpenAI(model="gpt-3.5-turbo")
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is AI?")
]
result = chat_model.invoke(messages)
```

#### å¼‚æ­¥æ”¯æŒ
æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒåŸç”Ÿå¼‚æ­¥æ“ä½œï¼š
```python
# å¼‚æ­¥è°ƒç”¨
result = await llm.ainvoke("Hello, world!")

# å¼‚æ­¥æµå¼
async for chunk in llm.astream("Tell me a story"):
    print(chunk, end="", flush=True)

# æ‰¹é‡å¼‚æ­¥å¤„ç†
tasks = [llm.ainvoke(prompt) for prompt in prompts]
results = await asyncio.gather(*tasks)
```

#### ç»“æ„åŒ–è¾“å‡º
```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

class AnalysisResult(BaseModel):
    summary: str = Field(description="å›ç­”æ‘˜è¦")
    confidence: float = Field(description="ç½®ä¿¡åº¦")

parser = JsonOutputParser(pydantic_object=AnalysisResult)
chain = prompt | chat_model | parser
result = chain.invoke({"text": "AI is a technology"})
```

### Prompts ç»„ä»¶

#### æ¨¡æ¿ç±»å‹
```python
# åŸºç¡€æç¤ºæ¨¡æ¿
prompt = PromptTemplate(
    template="Answer: {question}",
    input_variables=["question"]
)

# èŠå¤©æç¤ºæ¨¡æ¿
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "{question}")
])
)

# Few-shot æç¤ºæ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    prefix="Here are some examples:",
    suffix="Now answer: {input}"
)
```

#### è¾“å‡ºè§£æå™¨
```python
from langchain_core.output_parsers import (
    StrOutputParser, JsonOutputParser,
    PydanticOutputParser, ListOutputParser
)

parser = StrOutputParser()
chain = prompt | llm | parser
```

### Chains ç»„ä»¶ï¼ˆLCEL æ–¹å¼ï¼‰

#### æ›¿ä»£ä¼ ç»Ÿ Chain ç±»
```python
# âŒ æ—§æ–¹å¼ (å·²åºŸå¼ƒ)
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)

# âœ… æ–°æ–¹å¼ (LCEL)
chain = prompt | llm | StrOutputParser()
```

#### å¤æ‚å¤„ç†æ¨¡å¼
```python
# é¡ºåºå¤„ç†
chain = (
    RunnablePassthrough.assign(
        step1=step1_prompt | llm,
        step2=RunnablePassthrough.assign(
            step2=lambda x: step2_prompt.format(text=x["step1"])
        ) | llm
    )
    | RunnablePassthrough.assign(
        final=step3_prompt | llm
    )
)

# æ¡ä»¶å¤„ç†
def route_logic(inputs):
    if inputs["difficulty"] == "hard":
        return detailed_chain
    else:
        return simple_chain

router = RunnableLambda(route_logic)
chain = router | other_components

# é”™è¯¯å¤„ç†å’Œé‡è¯•
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2))
def robust_llm_call(prompt):
    return llm.invoke(prompt)
```

### Memory ç»„ä»¶

#### ä¼ ç»Ÿå†…å­˜ç±»å‹
```python
# å¯¹è¯ç¼“å­˜å†…å­˜
from langchain_classic.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    chat_history=[
        ("human", "Hello!"),
        ("ai", "Hi there!"),
        ("human", "What's your name?"),
        ("ai", "I'm a helpful assistant.")
    ]
)

# çª—å£å†…å­˜
from langchain_classic.memory import ConversationBufferWindowMemory
window_memory = ConversationBufferWindowMemory(k=2)

# æ‘˜è¦å†…å­˜
from langchain_classic.memory import ConversationSummaryMemory
summary_memory = ConversationSummaryMemory(llm=llm)

# çŸ¥è¯†å›¾è°±å†…å­˜
from langchain_community.memory.kg import ConversationKGMemory
```

#### æ–°çš„ ChatMessageHistory
```python
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import (
    ChatMessageHistory,
    FileChatMessageHistory
)

# è‡ªå®šä¹‰å†å²ç®¡ç†
class InMemoryChatMessageHistory(BaseChatMessageHistory):
    def __init__(self):
        self.messages = []

    def add_user_message(self, message: str):
        self.messages.append(("user", message))

    def add_ai_message(self, message: str):
        self.messages.append(("ai", message))

    def clear(self):
        self.messages = []
```

### Indexes ç»„ä»¶

#### æ–‡æ¡£åŠ è½½å™¨
```python
from langchain_community.document_loaders import (
    TextLoader, CSVLoader, JSONLoader, WebBaseLoader
)

# æ–‡æœ¬æ–‡ä»¶
loader = TextLoader("document.txt")
documents = loader.load()

# CSVæ–‡ä»¶
csv_loader = CSVLoader("data.csv")
csv_docs = csv_loader.load()

# ç½‘é¡µ
web_loader = WebBaseLoader("https://example.com")
web_docs = web_loader.load()
```

#### æ–‡æœ¬åˆ†å‰²å™¨
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# æŒ‰å­—ç¬¦åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

chunks = text_splitter.split_documents(documents)
```

#### å‘é‡å­˜å‚¨
```python
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings

# FAISS å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)

# ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search("Python programming", k=3)

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever()
retrieved_docs = retriever.invoke("machine learning")
```

### Agents ç»„ä»¶

#### æ™ºèƒ½ä½“åˆ›å»ºï¼ˆæ¨èæ–¹å¼ï¼‰
```python
from langchain_classic.agents import create_openai_tools_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools import Tool

# åˆ›å»ºå·¥å…·
tools = [
    Tool(
        name="calculator",
        description="Useful for math calculations",
        func=lambda x: str(eval(x))
    )
]

# åˆ›å»ºæ™ºèƒ½ä½“
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("human", "{input}"),
    ("assistant", "{agent_scratchpad}"),
    ("tool_call", "{observation}"),
    ("final", "{final_answer}")
])

agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# æ‰§è¡Œæ™ºèƒ½ä½“
result = agent_executor.invoke({"input": "Calculate 2 + 2"})
```

### Tools ç»„ä»¶

#### å†…ç½®å·¥å…·
```python
from langchain_community.tools import (
    WikipediaQueryRun, DuckDuckGoSearchRun,
    ShellTool, PythonREPLTool
)

# Wikipedia æœç´¢
wiki_tool = WikipediaQueryRun()

# DuckDuckGo æœç´¢
search_tool = DuckDuckGoSearchRun()

# Shell å·¥å…·ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
shell_tool = ShellTool()

# Python REPL å·¥å…·
python_tool = PythonREPLTool()
```

#### è‡ªå®šä¹‰å·¥å…·
```python
from langchain_classic.tools import tool
from pydantic import BaseModel, Field

@tool
def calculator(expression: str) -> str:
    """Performs basic math calculations."""
    try:
        return str(eval(expression))
    except:
        return "Calculation error"

# ä½¿ç”¨ Pydantic çš„å·¥å…·
class WeatherInput(BaseModel):
    location: str = Field(description="City name")
    unit: str = Field(default="celsius", description="Temperature unit")

@tool
def get_weather(input: WeatherInput) -> str:
    """Get current weather information."""
    # å®é™…çš„å¤©æ°”APIè°ƒç”¨
    return f"Weather in {input.location}: 25Â°C {input.unit}"
```

### Callbacks ç»„ä»¶

#### å›è°ƒå¤„ç†å™¨
```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult

class CustomCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized: Dict[str, Any], **kwargs):
        print(f"LLM å¼€å§‹: {serialized}")

    def on_llm_end(self, response: LLMResult, **kwargs):
        print(f"LLM ç»“æŸ: {response.generations[0].text}")

    def on_chain_start(self, serialized: Dict[str, Any], **kwargs):
        print(f"Chain å¼€å§‹: {serialized}")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs):
        print(f"Chain ç»“æŸ: {outputs}")

# ä½¿ç”¨å›è°ƒ
handler = CustomCallbackHandler()
chain = prompt | llm | output_parser
result = chain.invoke(
    {"question": "What is AI?"},
    callbacks=[handler]
)
```

#### å¼‚æ­¥å›è°ƒ
```python
from langchain_core.callbacks import AsyncCallbackHandler

class AsyncCallbackHandler(AsyncCallbackHandler):
    async def on_llm_start(self, serialized, **kwargs):
        print(f"å¼‚æ­¥ LLM å¼€å§‹")

    async def on_llm_end(self, response, **kwargs):
        print(f"å¼‚æ­¥ LLM ç»“æŸ")

# æµå¼å›è°ƒ
class StreamingCallbackHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        print(token, end="", flush=True)
```

## ğŸ”§ æœ€ä½³å®è·µ

### 1. LCEL é“¾å¼ç»„åˆ
```python
# âœ… æ¨èï¼šä½¿ç”¨ LCEL
chain = (
    {"context": retriever}
    | RunnablePassthrough.assign(
        answer=prompt | llm | StrOutputParser()
    )
)

# âŒ é¿å…ï¼šä¼ ç»Ÿçš„ Chain ç±»
# chain = LLMChain(llm=llm, prompt=prompt)
```

### 2. å¼‚æ­¥ä¼˜å…ˆ
```python
# âœ… æ¨èï¼šå¼‚æ­¥æ“ä½œ
async def process_batch():
    tasks = [chain.ainvoke(item) for item in items]
    results = await asyncio.gather(*tasks)
    return results

# âŒ é¿å…ï¼šåŒæ­¥é˜»å¡ï¼ˆåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ï¼‰
def process_batch():
    results = []
    for item in items:
        results.append(chain.invoke(item))  # é˜»å¡æ“ä½œ
    return results
```

### 3. ç±»å‹å®‰å…¨
```python
from typing import Dict, Any, List

def process_input(input_data: Dict[str, Any]) -> str:
    """ç±»å‹å®‰å…¨çš„å¤„ç†å‡½æ•°"""
    return chain.invoke(input_data)
```

### 4. é”™è¯¯å¤„ç†
```python
from langchain_core.exceptions import LangChainException
from tenacity import retry, stop_after_attempt

@retry(stop=stop_after_attempt(3))
@retry(wait=wait_exponential(multiplier=2))
def robust_chain_call(input_data: Dict[str, Any]) -> str:
    try:
        return chain.invoke(input_data)
    except LangChainException as e:
        logger.error(f"Chain æ‰§è¡Œå¤±è´¥: {e}")
        raise
```

### 5. èµ„æºç®¡ç†
```python
# ç¼“å­˜æé«˜æ€§èƒ½
from langchain_core.caches import InMemoryCache
from langchain_core.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# æµå¼è¾“å‡ºå‡å°‘å†…å­˜ä½¿ç”¨
for chunk in chain.stream(input_data):
    process_chunk(chunk)
```

## ğŸ“š å­¦ä¹ è·¯å¾„å»ºè®®

### åˆçº§é˜¶æ®µ
1. **æŒæ¡ LCEL åŸºç¡€**ï¼šå­¦ä¹  pipe operator å’ŒåŸºæœ¬ç»„ä»¶
2. **å¼‚æ­¥ç¼–ç¨‹**ï¼šç†è§£ async/await åœ¨ LangChain ä¸­çš„åº”ç”¨
3. **é”™è¯¯å¤„ç†**ï¼šå­¦ä¹ å¦‚ä½•ä¼˜é›…åœ°å¤„ç†å¼‚å¸¸

### ä¸­çº§é˜¶æ®µ
1. **å¤æ‚æ•°æ®æµ**ï¼šä½¿ç”¨ RunnablePassthroughã€RunnableParallel
2. **è‡ªå®šä¹‰ç»„ä»¶**ï¼šåˆ›å»ºè‡ªå·±çš„å·¥å…·ã€å›è°ƒå¤„ç†å™¨
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼“å­˜ã€æ‰¹é‡å¤„ç†ã€å¼‚æ­¥è°ƒç”¨

### é«˜çº§é˜¶æ®µ
1. **è‡ªå®šä¹‰ LLM**ï¼šå®ç°è‡ªå·±çš„æ¨¡å‹æ¥å£
2. **é«˜çº§ Agent æ¶æ„**ï¼šå¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡
3. **ç³»ç»Ÿé›†æˆ**ï¼šä¸å¤–éƒ¨æœåŠ¡çš„æ·±åº¦é›†æˆ

## ğŸ”— å‚è€ƒèµ„æº

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LCEL æŒ‡å—](https://python.langchain.com/docs/concepts/lcel/)
- [LangChain 1.x è¿ç§»æŒ‡å—](./LANGCHAIN_1X_MIGRATION_GUIDE.md)
- [ç¤ºä¾‹ä»£ç ä»“åº“](https://github.com/langchain-ai/langchain/tree/master/examples)

---

ğŸ’¡ **æ ¸å¿ƒè¦ç‚¹**ï¼šLangChain 1.x çš„æ ¸å¿ƒæ˜¯ **LCEL**ï¼ˆLangChain Expression Languageï¼‰å’Œ**æ¨¡å—åŒ–æ¶æ„**ã€‚æŒæ¡è¿™ä¸¤ä¸ªæ¦‚å¿µï¼Œå°±èƒ½æ„å»ºå‡ºå¼ºå¤§ã€å¯ç»´æŠ¤çš„ LLM åº”ç”¨ã€‚