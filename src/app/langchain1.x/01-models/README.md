# LangChain Models ç»„ä»¶å­¦ä¹ æŒ‡å—

Modelsæ˜¯LangChainæ¡†æ¶ä¸­æœ€åŸºç¡€çš„ç»„ä»¶ï¼Œè´Ÿè´£ä¸å„ç§è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»Modelsç»„ä»¶çš„æ ¸å¿ƒæ¦‚å¿µã€ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‹ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. Modelsç»„ä»¶åˆ†ç±»

#### 1.1 LLMs (Large Language Models)
- **å®šä¹‰**ï¼šçº¯æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œè¾“å…¥å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå­—ç¬¦ä¸²
- **ç‰¹ç‚¹**ï¼šæ— çŠ¶æ€ã€ç®€å•ç›´æ¥
- **ä½¿ç”¨åœºæ™¯**ï¼šæ–‡æœ¬è¡¥å…¨ã€ç¿»è¯‘ã€æ‘˜è¦ç­‰

#### 1.2 Chat Models
- **å®šä¹‰**ï¼šåŸºäºå¯¹è¯çš„æ¨¡å‹ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥è¾“å‡º
- **ç‰¹ç‚¹**ï¼šæ”¯æŒç³»ç»Ÿæ¶ˆæ¯ã€è§’è‰²åŒºåˆ†ã€ä¸Šä¸‹æ–‡ç®¡ç†
- **ä½¿ç”¨åœºæ™¯**ï¼šå¯¹è¯ç³»ç»Ÿã€è§’è‰²æ‰®æ¼”ã€å¤šè½®äº¤äº’

#### 1.3 Text Embedding Models
- **å®šä¹‰**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºçš„æ¨¡å‹
- **ç‰¹ç‚¹**ï¼šé«˜ç»´å‘é‡ã€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
- **ä½¿ç”¨åœºæ™¯**ï¼šè¯­ä¹‰æœç´¢ã€æ–‡æ¡£èšç±»ã€æ¨èç³»ç»Ÿ

### 2. æ ¸å¿ƒæ¥å£è®¾è®¡

#### 2.1 BaseLLMæ¥å£
```python
class BaseLLM(BaseLanguageModel[str]):
    def _generate(self, prompts: List[str], **kwargs) -> LLMResult
    def _llm_type(self) -> str
```

#### 2.2 BaseChatModelæ¥å£
```python
class BaseChatModel(BaseLanguageModel[BaseMessage]):
    def _generate(self, messages: List[List[BaseMessage]], **kwargs) -> ChatResult
    def _llm_type(self) -> str
```

#### 2.3 BaseEmbeddingsæ¥å£
```python
class BaseEmbeddings(ABC):
    def embed_documents(self, texts: List[str]) -> List[List[float]]
    def embed_query(self, text: str) -> List[float]
```

### 3. æµå¼è¾“å‡ºä¸å¼‚æ­¥æ”¯æŒ

#### 3.1 æµå¼è¾“å‡º
- æ”¯æŒé€tokenç”Ÿæˆ
- å®æ—¶å“åº”æå‡ç”¨æˆ·ä½“éªŒ
- é€‚ç”¨äºé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯

#### 3.2 å¼‚æ­¥è°ƒç”¨
- æ”¯æŒasync/awaitè¯­æ³•
- æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›
- é€‚ç”¨äºé«˜å¹¶å‘åº”ç”¨

## ğŸ¯ å¸¸è§é¢è¯•é¢˜

### åŸºç¡€æ¦‚å¿µé¢˜

**Q1: LangChainä¸­çš„LLMå’ŒChat Modelæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

**A1:**
- **è¾“å…¥è¾“å‡ºæ ¼å¼**ï¼šLLMæ¥å—å­—ç¬¦ä¸²è¾“å…¥è¾“å‡ºå­—ç¬¦ä¸²ï¼ŒChat Modelæ¥å—æ¶ˆæ¯åˆ—è¡¨è¾“å…¥è¾“å‡ºæ¶ˆæ¯åˆ—è¡¨
- **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šChat Modelå¤©ç„¶æ”¯æŒå¤šè½®å¯¹è¯å’Œè§’è‰²åŒºåˆ†ï¼ŒLLMéœ€è¦æ‰‹åŠ¨ç®¡ç†ä¸Šä¸‹æ–‡
- **åŠŸèƒ½ç‰¹æ€§**ï¼šChat Modelé€šå¸¸æ”¯æŒç³»ç»Ÿæ¶ˆæ¯ã€åŠŸèƒ½è°ƒç”¨ç­‰é«˜çº§ç‰¹æ€§
- **ä½¿ç”¨åœºæ™¯**ï¼šLLMé€‚åˆç®€å•çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒChat Modelé€‚åˆå¤æ‚çš„å¯¹è¯åœºæ™¯

**Q2: ä»€ä¹ˆæ˜¯Text Embeddingï¼Œå®ƒåœ¨LangChainä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**

**A2:**
- **å®šä¹‰**ï¼šText Embeddingæ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´æ•°å€¼å‘é‡çš„æŠ€æœ¯
- **ä½œç”¨**ï¼š
  - è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
  - æ–‡æ¡£æ£€ç´¢å’Œæœç´¢
  - æ–‡æœ¬èšç±»å’Œåˆ†ç±»
  - æ¨èç³»ç»ŸåŸºç¡€
- **åœ¨LangChainä¸­**ï¼šä¸»è¦ç”¨äºVectorStoreså’ŒRetrieval Chainï¼Œå®ç°åŸºäºè¯­ä¹‰çš„æ–‡æ¡£æ£€ç´¢

### æŠ€æœ¯å®ç°é¢˜

**Q3: å¦‚ä½•å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„LLMåŒ…è£…å™¨ï¼Ÿ**

**A3:**
```python
from langchain_core.language_models.llms import BaseLLM
from typing import Optional, List, Any

class CustomLLM(BaseLLM):
    def __init__(self, api_key: str, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
    
    @property
    def _llm_type(self) -> str:
        return "custom_llm"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        response = call_custom_api(prompt, self.api_key)
        return response
    
    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop, run_manager, **kwargs)
            generations.append([Generation(text=text)])
        return LLMResult(generations=generations)
```

**Q4: å¦‚ä½•å¤„ç†LLMçš„æµå¼è¾“å‡ºï¼Ÿ**

**A4:**
```python
from langchain_core.callbacks import StreamingStdOutCallbackHandler

streaming_handler = StreamingStdOutCallbackHandler()
llm = OpenAI(streaming=True, callbacks=[streaming_handler])

for chunk in llm.stream("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"):
    print(chunk.content, end="", flush=True)

async def stream_response():
    async for chunk in llm.astream("è§£é‡Šé‡å­è®¡ç®—"):
        print(chunk.content, end="", flush=True)
```

### æ¶æ„è®¾è®¡é¢˜

**Q5: LangChainçš„Modelsç»„ä»¶é‡‡ç”¨äº†ä»€ä¹ˆè®¾è®¡æ¨¡å¼ï¼Ÿ**

**A5:**
- **é€‚é…å™¨æ¨¡å¼**ï¼šå°†ä¸åŒLLMæä¾›å•†çš„APIç»Ÿä¸€ä¸ºç›¸åŒæ¥å£
- **ç­–ç•¥æ¨¡å¼**ï¼šæ”¯æŒä¸åŒçš„æ¨¡å‹é€‰æ‹©å’Œé…ç½®ç­–ç•¥
- **æ¨¡æ¿æ–¹æ³•æ¨¡å¼**ï¼šBaseLLMå®šä¹‰ç®—æ³•éª¨æ¶ï¼Œå­ç±»å®ç°å…·ä½“ç»†èŠ‚
- **å·¥å‚æ¨¡å¼**ï¼šé€šè¿‡from_pretrainedç­‰æ–¹æ³•åˆ›å»ºæ¨¡å‹å®ä¾‹

## ğŸ—ï¸ è®¾è®¡æ€è·¯å’Œè®¾è®¡æ¨¡å¼

### 1. ç»Ÿä¸€æ¥å£è®¾è®¡

#### 1.1 é€‚é…å™¨æ¨¡å¼åº”ç”¨
LangChainé€šè¿‡é€‚é…å™¨æ¨¡å¼è§£å†³äº†ä¸åŒLLMæä¾›å•†APIå·®å¼‚çš„é—®é¢˜ï¼š

```python
llm = OpenAI()
llm = Anthropic()
llm = HuggingFaceHub()

result = llm("Hello, world!")
```

#### 1.2 æŠ½è±¡å·¥å‚æ¨¡å¼
é€šè¿‡æŠ½è±¡å·¥å‚æ¨¡å¼æ”¯æŒä¸åŒç±»å‹çš„æ¨¡å‹åˆ›å»ºï¼š

```python
class ModelFactory:
    @staticmethod
    def create_llm(provider: str, **kwargs) -> BaseLLM:
        if provider == "openai":
            return OpenAI(**kwargs)
        elif provider == "anthropic":
            return Anthropic(**kwargs)
```

### 2. æ‰©å±•æ€§è®¾è®¡

#### 2.1 æ’ä»¶åŒ–æ¶æ„
- é€šè¿‡ç»§æ‰¿åŸºç±»è½»æ¾æ·»åŠ æ–°çš„æ¨¡å‹æ”¯æŒ
- é…ç½®é©±åŠ¨çš„æ¨¡å‹é€‰æ‹©
- åŠ¨æ€åŠ è½½æ¨¡å‹æ’ä»¶

#### 2.2 ä¸­é—´ä»¶æ¨¡å¼
æ”¯æŒåœ¨æ¨¡å‹è°ƒç”¨å‰åæ·»åŠ ä¸­é—´ä»¶å¤„ç†ï¼š

```python
class LoggingMiddleware:
    def __call__(self, llm, prompt, **kwargs):
        logger.info(f"Input: {prompt}")
        result = llm(prompt, **kwargs)
        logger.info(f"Output: {result}")
        return result
```

### 3. æ€§èƒ½ä¼˜åŒ–è®¾è®¡

#### 3.1 ç¼“å­˜æœºåˆ¶
```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())
```

#### 3.2 æ‰¹å¤„ç†ä¼˜åŒ–
```python
prompts = ["prompt1", "prompt2", "prompt3"]
results = llm.generate(prompts)
```

#### 3.3 å¼‚æ­¥æ”¯æŒ
```python
import asyncio

async def parallel_calls():
    tasks = [llm.ainvoke(f"prompt {i}") for i in range(5)]
    results = await asyncio.gather(*tasks)
    return results
```

## ğŸš€ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

1. **ä»»åŠ¡åŒ¹é…**ï¼šæ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç±»å‹
2. **æˆæœ¬è€ƒè™‘**ï¼šå¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œä½¿ç”¨æˆæœ¬
3. **å»¶è¿Ÿè¦æ±‚**ï¼šæ ¹æ®å®æ—¶æ€§è¦æ±‚é€‰æ‹©æ¨¡å‹
4. **å‡†ç¡®æ€§éœ€æ±‚**ï¼šå…³é”®ä»»åŠ¡ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹

### 2. é”™è¯¯å¤„ç†

```python
from langchain_core.exceptions import LangChainException
import time
from functools import wraps

def retry_with_backoff(max_retries=3, backoff_factor=2):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except LangChainException as e:
                    if attempt == max_retries - 1:
                        raise
                    wait_time = backoff_factor ** attempt
                    time.sleep(wait_time)
            return None
        return wrapper
    return decorator

@retry_with_backoff()
def safe_llm_call(prompt):
    return llm.invoke(prompt)
```

### 3. ç›‘æ§å’Œæ—¥å¿—

```python
from langchain_core.callbacks import get_openai_callback

def monitored_llm_call(prompt):
    with get_openai_callback() as cb:
        result = llm.invoke(prompt)
        print(f"Total Cost: ${cb.total_cost}")
        print(f"Total Tokens: {cb.total_tokens}")
        return result
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ç±»å‹ | å“åº”æ—¶é—´ | æˆæœ¬ | å‡†ç¡®æ€§ | é€‚ç”¨åœºæ™¯ |
|---------|---------|------|--------|----------|
| GPT-4 | æ…¢ | é«˜ | å¾ˆé«˜ | å¤æ‚æ¨ç†ã€åˆ›ä½œ |
| GPT-3.5-Turbo | å¿« | ä¸­ | é«˜ | å¯¹è¯ã€é€šç”¨ä»»åŠ¡ |
| Claude | ä¸­ | ä¸­ | é«˜ | é•¿æ–‡æœ¬å¤„ç† |
| LLaMA | ä¸­-å¿« | ä½-ä¸­ | ä¸­-é«˜ | æœ¬åœ°éƒ¨ç½² |

## ğŸ”— ç›¸å…³èµ„æº

- [LangChain Modelså®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/model_io/models/)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [HuggingFaceæ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)

---

ğŸ’¡ **å­¦ä¹ å»ºè®®**ï¼šå»ºè®®ä»åŸºç¡€çš„LLMå¼€å§‹å­¦ä¹ ï¼Œç„¶åé€æ­¥æŒæ¡Chat Modelå’ŒEmbeddingsï¼Œæœ€åå°è¯•è‡ªå®šä¹‰æ¨¡å‹å®ç°ã€‚