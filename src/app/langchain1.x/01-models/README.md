# LangChain Models ç»„ä»¶å­¦ä¹ æŒ‡å— (LangChain 1.x ç‰ˆæœ¬)

Modelsæ˜¯LangChainæ¡†æ¶ä¸­æœ€åŸºç¡€çš„ç»„ä»¶ï¼Œè´Ÿè´£ä¸å„ç§è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»Modelsç»„ä»¶åœ¨LangChain 1.xä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‹ LangChain 1.x æ ¸å¿ƒå˜åŒ–

### å¯¼å…¥è·¯å¾„å˜åŒ–
- **ä» langchain åˆ° langchain_core**: åŸºç¡€æ¥å£å’Œæ¶ˆæ¯ç±»å·²ç§»è‡³ `langchain_core`
- **ä¸“é—¨åŒ…**: æä¾›å•†ç‰¹å®šçš„åŒ…å¦‚ `langchain_openai`, `langchain_community`
- **æ¨¡å—åŒ–æ¶æ„**: æ ¸å¿ƒåŠŸèƒ½ä¸ç¤¾åŒºæ’ä»¶åˆ†ç¦»

### æ–°ç‰¹æ€§
- **å¼‚æ­¥æ”¯æŒ**: æ‰€æœ‰çš„æ¨¡å‹éƒ½æ”¯æŒå¼‚æ­¥è°ƒç”¨
- **LCELå…¼å®¹**: ä¸LangChain Expression Languageå®Œå…¨å…¼å®¹
- **æ”¹è¿›çš„é”™è¯¯å¤„ç†**: æ›´å¥½çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶

## ğŸ¯ Modelsç»„ä»¶åˆ†ç±»

### 1. LLMs (Large Language Models)

#### LangChain 1.x ä¸­çš„å˜åŒ–
- ä» `langchain.llms` ç§»è‡³ `langchain_openai.OpenAI`
- æ¨èä½¿ç”¨ `ChatOpenAI` æ›¿ä»£ä¼ ç»Ÿ LLM
- å¢å¼ºçš„æµå¼è¾“å‡ºæ”¯æŒ

```python
# LangChain 1.x æ¨èå†™æ³•
from langchain_openai import OpenAI

llm = OpenAI(
    model="gpt-3.5-turbo-instruct",
    temperature=0.7
)

# æ”¯æŒæµå¼è¾“å‡º
for chunk in llm.stream("å†™ä¸€é¦–è¯—"):
    print(chunk, end="")

# æ”¯æŒå¼‚æ­¥è°ƒç”¨
result = await llm.ainvoke("Hello, world!")
```

### 2. Chat Models

#### LangChain 1.x æ–°ç‰¹æ€§
- ç»“æ„åŒ–è¾“å‡ºæ”¯æŒ
- æ›´å¥½çš„å¼‚æ­¥æµå¼è¾“å‡º
- ä¸ LCEL å®Œå…¨å…¼å®¹
- **ç»Ÿä¸€æ¨¡å‹åˆå§‹åŒ–** (`init_chat_model`)

```python
# æ–¹æ³•1: ä¼ ç»Ÿæ–¹å¼ (ä»ç„¶æ”¯æŒ)
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºèŠå¤©æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-3.5-turbo")

# ä½¿ç”¨ LCEL åˆ›å»ºé“¾
prompt = ChatPromptTemplate.from_template("è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š{question}")
chain = prompt | chat_model | StrOutputParser()

# æ‰§è¡Œ
result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"})
```

#### init_chat_model - LangChain 1.x æ¨èæ–¹å¼

```python
# æ–¹æ³•2: init_chat_model (æ¨è)
from langchain.chat_models import init_chat_model

# æœ€ç®€å•çš„åˆå§‹åŒ– - è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
model = init_chat_model("gpt-4")

# å¸¦å‚æ•°çš„åˆå§‹åŒ–
model = init_chat_model(
    "gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=100
)

# æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†
openai_model = init_chat_model("gpt-4")           # OpenAI
anthropic_model = init_chat_model("claude-3")     # Anthropic (éœ€è¦ langchain-anthropic)
google_model = init_chat_model("gemini-pro")      # Google (éœ€è¦ langchain-google-genai)

# å®Œå…¨ç›¸åŒçš„è°ƒç”¨æ–¹å¼
response = model.invoke([HumanMessage(content="ä½ å¥½ï¼")])
```

#### ç»“æ„åŒ–è¾“å‡º (æ–°ç‰¹æ€§)
```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

class Answer(BaseModel):
    summary: str = Field(description="å›ç­”æ‘˜è¦")
    details: List[str] = Field(description="è¯¦ç»†è¦ç‚¹")

parser = JsonOutputParser(pydantic_object=Answer)
chain = prompt | chat_model | parser
result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"})
```

### 3. Text Embedding Models

#### LangChain 1.x å˜åŒ–
- æ”¯æŒæœ€æ–°çš„ embedding æ¨¡å‹
- å¼‚æ­¥åµŒå…¥ç”Ÿæˆ
- æ›´å¥½çš„æ‰¹å¤„ç†æ”¯æŒ

```python
# LangChain 1.x æ¨èå†™æ³•
from langchain_openai import OpenAIEmbeddings

# ä½¿ç”¨æœ€æ–°æ¨¡å‹
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# å¼‚æ­¥åµŒå…¥ç”Ÿæˆ
vector = await embeddings.aembed_query("æŸ¥è¯¢æ–‡æœ¬")

# æ‰¹é‡å¼‚æ­¥å¤„ç†
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
tasks = [embeddings.aembed_query(text) for text in texts]
vectors = await asyncio.gather(*tasks)
```

## ğŸ”§ æ ¸å¿ƒæ¥å£è®¾è®¡

### 1. BaseLLMæ¥å£ (langchain_core)
```python
from langchain_core.language_models.llms import BaseLLM

class CustomLLM(BaseLLM):
    @property
    def _llm_type(self) -> str:
        return "custom_llm"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        # å®ç°
        pass

    async def _acall(self, prompt: str, **kwargs) -> str:
        # å¼‚æ­¥å®ç° (LangChain 1.x æ–°å¢)
        pass
```

### 2. BaseChatModelæ¥å£ (langchain_core)
```python
from langchain_core.language_models.chat_models import BaseChatModel

class CustomChatModel(BaseChatModel):
    def _generate(
        self,
        messages: List[List[BaseMessage]],
        **kwargs: Any,
    ) -> ChatResult:
        # å®ç°
        pass

    async def _agenerate(
        self,
        messages: List[List[BaseMessage]],
        **kwargs: Any,
    ) -> ChatResult:
        # å¼‚æ­¥å®ç° (LangChain 1.x æ–°å¢)
        pass
```

### 3. BaseEmbeddingsæ¥å£ (langchain_core)
```python
from langchain_core.embeddings import Embeddings

class CustomEmbeddings(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # å®ç°
        pass

    def embed_query(self, text: str) -> List[float]:
        # å®ç°
        pass

    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        # å¼‚æ­¥å®ç° (LangChain 1.x æ–°å¢)
        pass

    async def aembed_query(self, text: str) -> List[float]:
        # å¼‚æ­¥å®ç° (LangChain 1.x æ–°å¢)
        pass
```

## ğŸš€ LangChain 1.x æ–°ç‰¹æ€§

### 1. å¼‚æ­¥æ”¯æŒ
æ‰€æœ‰æ¨¡å‹ç°åœ¨éƒ½æ”¯æŒå¼‚æ­¥æ“ä½œï¼š

```python
import asyncio
from langchain_openai import ChatOpenAI

chat_model = ChatOpenAI()

# å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
async def process_questions():
    questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
    tasks = [chat_model.ainvoke([HumanMessage(content=q)]) for q in questions]
    results = await asyncio.gather(*tasks)
    return results

results = asyncio.run(process_questions())
```

### 2. æ”¹è¿›çš„æµå¼è¾“å‡º
```python
# åŒæ­¥æµå¼
for chunk in chat_model.stream(messages):
    print(chunk.content, end="")

# å¼‚æ­¥æµå¼
async for chunk in chat_model.astream(messages):
    print(chunk.content, end="")
```

### 3. LCEL é›†æˆ
```python
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# åˆ›å»ºå¤æ‚çš„å¤„ç†é“¾
chain = (
    RunnablePassthrough.assign(
        embedding=lambda x: embeddings.embed_query(x["text"])
    ) | RunnableParallel({
        "summary": summary_prompt | chat_model | StrOutputParser(),
        "keywords": keywords_prompt | chat_model | StrOutputParser()
    })
)
```

## ğŸ¯ å¸¸è§é¢è¯•é¢˜ (LangChain 1.x ç‰ˆæœ¬)

### åŸºç¡€æ¦‚å¿µé¢˜

**Q1: LangChain 1.x ä¸­ Models ç»„ä»¶çš„ä¸»è¦å˜åŒ–æ˜¯ä»€ä¹ˆï¼Ÿ**

**A1:**
- **å¯¼å…¥è·¯å¾„å˜åŒ–**: ä» `langchain` ç§»è‡³ `langchain_core` å’Œä¸“é—¨åŒ…
- **å¼‚æ­¥æ”¯æŒ**: æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ `ainvoke()`, `astream()`, `abatch()` ç­‰å¼‚æ­¥æ–¹æ³•
- **LCELå…¼å®¹**: å®Œå…¨æ”¯æŒ LangChain Expression Language
- **ç»“æ„åŒ–è¾“å‡º**: Chat Models æ”¯æŒåŸç”Ÿç»“æ„åŒ–è¾“å‡º
- **æ”¹è¿›çš„é”™è¯¯å¤„ç†**: æ›´å¥½çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶

**Q2: å¦‚ä½•åœ¨ LangChain 1.x ä¸­å®ç°è‡ªå®šä¹‰LLMï¼Ÿ**

**A2:**
```python
from langchain_core.language_models.llms import BaseLLM
from langchain_core.callbacks import CallbackManagerForLLMRun

class CustomLLM(BaseLLM):
    def _call(self, prompt: str, stop=None, run_manager=None, **kwargs):
        # åŒæ­¥å®ç°
        return response

    async def _acall(self, prompt: str, stop=None, run_manager=None, **kwargs):
        # å¼‚æ­¥å®ç° (LangChain 1.x æ–°å¢)
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        return response

    @property
    def _llm_type(self) -> str:
        return "custom_llm"
```

### æŠ€æœ¯å®ç°é¢˜

**Q3: å¦‚ä½•åœ¨ LangChain 1.x ä¸­å®ç°ç»“æ„åŒ–è¾“å‡ºï¼Ÿ**

**A3:**
```python
from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List

# å®šä¹‰è¾“å‡ºç»“æ„
class AnalysisResult(BaseModel):
    summary: str = Field(description="æ‘˜è¦")
    key_points: List[str] = Field(description="å…³é”®è¦ç‚¹")
    sentiment: str = Field(description="æƒ…æ„Ÿå€¾å‘")

# åˆ›å»ºè§£æå™¨
parser = JsonOutputParser(pydantic_object=AnalysisResult)

# åˆ›å»ºé“¾
prompt = ChatPromptTemplate.from_template(
    "åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š{text}\n\n{format_instructions}"
)
chain = prompt | chat_model | parser

# æ‰§è¡Œ
result = chain.invoke({
    "text": "è¦åˆ†æçš„æ–‡æœ¬",
    "format_instructions": parser.get_format_instructions()
})
```

**Q4: å¦‚ä½•å¤„ç†æ¨¡å‹çš„å¼‚æ­¥è°ƒç”¨å’Œé”™è¯¯é‡è¯•ï¼Ÿ**

**A4:**
```python
import asyncio
from functools import wraps
from langchain_core.exceptions import LangChainException

def retry_with_backoff(max_retries=3, backoff_factor=2.0):
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except LangChainException as e:
                    if attempt == max_retries - 1:
                        raise
                    await asyncio.sleep(backoff_factor ** attempt)
        return async_wrapper
    return decorator

@retry_with_backoff()
async def robust_llm_call(prompt: str):
    return await chat_model.ainvoke([HumanMessage(content=prompt)])
```

### æ¶æ„è®¾è®¡é¢˜

**Q5: LangChain 1.x çš„ Models ç»„ä»¶æ¶æ„è®¾è®¡æœ‰ä»€ä¹ˆæ”¹è¿›ï¼Ÿ**

**A5:**
- **æ¨¡å—åŒ–åˆ†ç¦»**: æ ¸å¿ƒæ¥å£åœ¨ `langchain_core`ï¼Œå®ç°åœ¨ä¸“é—¨åŒ…ä¸­
- **å¼‚æ­¥ä¼˜å…ˆ**: ä»è®¾è®¡å±‚é¢æ”¯æŒå¼‚æ­¥æ“ä½œ
- **LCELé›†æˆ**: ä½œä¸ºå¯è¿è¡Œç»„ä»¶å®Œå…¨é›†æˆåˆ°è¡¨è¾¾è¯­è¨€ä¸­
- **ç±»å‹å®‰å…¨**: æ›´å¥½çš„ç±»å‹æç¤ºå’ŒéªŒè¯
- **æ’ä»¶åŒ–**: æ›´å®¹æ˜“æ·»åŠ æ–°çš„æ¨¡å‹æä¾›å•†

## ğŸ—ï¸ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥
```python
# LangChain 1.x æ¨èçš„æ¨¡å‹é€‰æ‹©
def get_model(use_chat=True, use_async=False):
    if use_chat:
        return ChatOpenAI(model="gpt-3.5-turbo")
    else:
        return OpenAI(model="gpt-3.5-turbo-instruct")

# æ‰¹é‡å¤„ç†ä¼˜åŒ–
async def batch_process(texts):
    embeddings = OpenAIEmbeddings()
    tasks = [embeddings.aembed_query(text) for text in texts]
    return await asyncio.gather(*tasks)
```

### 2. æ€§èƒ½ä¼˜åŒ–
```python
# ä½¿ç”¨ LCEL ä¼˜åŒ–é“¾å¼è°ƒç”¨
optimized_chain = (
    RunnableParallel({
        "text": lambda x: x["input"],
        "embedding": embeddings | RunnableLambda(lambda e: e)
    })
    | RunnablePassthrough.assign(
        summary=summary_prompt | chat_model | StrOutputParser()
    )
)

# å¼‚æ­¥æ‰¹å¤„ç†
async def process_batch(items):
    semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°

    async def process_single(item):
        async with semaphore:
            return await chat_model.ainvoke(item)

    tasks = [process_single(item) for item in items]
    return await asyncio.gather(*tasks)
```

### 3. é”™è¯¯å¤„ç†å’Œç›‘æ§
```python
from langchain_core.callbacks import get_openai_callback

async def monitored_call(prompt):
    with get_openai_callback() as cb:
        try:
            result = await chat_model.ainvoke(prompt)
            print(f"Cost: ${cb.total_cost:.6f}")
            return result
        except Exception as e:
            print(f"Error: {e}")
            raise
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯” (LangChain 1.x)

| ç‰¹æ€§ | LangChain 0.x | LangChain 1.x |
|------|---------------|---------------|
| å¼‚æ­¥æ”¯æŒ | æœ‰é™ | åŸç”Ÿæ”¯æŒ |
| ç»“æ„åŒ–è¾“å‡º | éœ€è¦æ‰‹åŠ¨è§£æ | åŸç”Ÿæ”¯æŒ |
| LCELé›†æˆ | éƒ¨åˆ†æ”¯æŒ | å®Œå…¨æ”¯æŒ |
| é”™è¯¯å¤„ç† | åŸºç¡€ | å¢å¼º |
| ç±»å‹å®‰å…¨ | åŸºç¡€ | å®Œå–„ |
| æµå¼è¾“å‡º | æ”¯æŒ | æ”¹è¿› |

## ğŸ”— ç›¸å…³èµ„æº

- [LangChain Models å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/model_io/models/)
- [LangChain 1.x è¿ç§»æŒ‡å—](https://python.langchain.com/docs/versions/migrating_to_lcel/)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [LangChain Expression Language æŒ‡å—](https://python.langchain.com/docs/concepts/lcel/)

## ğŸ“ ç¤ºä¾‹æ–‡ä»¶

- [`basic_example.py`](basic_example.py) - å®Œæ•´çš„åŸºç¡€ç¤ºä¾‹ï¼ŒåŒ…å«LLMã€Chat Modelså’ŒEmbeddings
- [`advanced_example.py`](advanced_example.py) - é«˜çº§ç‰¹æ€§ç¤ºä¾‹
- [`init_chat_model_example.py`](init_chat_model_example.py) - **æ–°å¢** - init_chat_modelç»Ÿä¸€åˆå§‹åŒ–æ–¹å¼ç¤ºä¾‹

### init_chat_model ç¤ºä¾‹æ–‡ä»¶ç‰¹æ€§

è¯¥ç¤ºä¾‹å±•ç¤ºäº† LangChain 1.x ä¸­æ¨èçš„æ¨¡å‹åˆå§‹åŒ–æ–¹å¼ï¼š

- **åŸºç¡€ä½¿ç”¨** - æœ€ç®€å•çš„æ¨¡å‹åˆå§‹åŒ–
- **å¤šæä¾›å•†æ”¯æŒ** - OpenAIã€Anthropicã€Googleç­‰
- **å‚æ•°é…ç½®** - temperatureã€max_tokensç­‰
- **æµå¼è¾“å‡º** - å®æ—¶å“åº”æµ
- **å¼‚æ­¥è°ƒç”¨** - å¹¶å‘å¤„ç†ç¤ºä¾‹
- **å¤šè½®å¯¹è¯** - å¯¹è¯å†å²ç®¡ç†
- **æ‰¹é‡å¤„ç†** - é«˜æ•ˆæ‰¹é‡è°ƒç”¨
- **ç»“æ„åŒ–è¾“å‡º** - JSONæ ¼å¼è¾“å‡º
- **é”™è¯¯å¤„ç†** - å¼‚å¸¸å¤„ç†æœ€ä½³å®è·µ
- **æ€§èƒ½å¯¹æ¯”** - ä¸åŒæ¨¡å‹æ€§èƒ½å¯¹æ¯”

---

ğŸ’¡ **å­¦ä¹ å»ºè®®**ï¼šå»ºè®®ä»åŸºç¡€çš„æ¨¡å‹ä½¿ç”¨å¼€å§‹å­¦ä¹ ï¼Œç„¶åæŒæ¡å¼‚æ­¥å’ŒLCELçš„é«˜çº§ç‰¹æ€§ï¼Œæœ€åå°è¯•è‡ªå®šä¹‰æ¨¡å‹å®ç°ã€‚åœ¨ LangChain 1.x ä¸­ï¼Œ**å¼‚æ­¥å¤„ç†**ã€**LCEL**å’Œ**init_chat_model**æ˜¯å…³é”®æŠ€èƒ½ã€‚